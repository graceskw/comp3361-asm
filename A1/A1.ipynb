{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.txt** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **1. clean all outputs and 2. run all cells in your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "# 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YsSAtTqt7Q8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The syntax of the command is incorrect.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/lm\n",
        "!wget -O data/lm/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/train.txt\n",
        "!wget -O data/lm/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/dev.txt\n",
        "!wget -O data/lm/test.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "## 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "q-rNT_QL8Dvt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary = 26601\n"
          ]
        }
      ],
      "source": [
        "with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    train = dict()\n",
        "    for line in f:\n",
        "        line = '<START> ' + line + ' <END>'     # to add <START> and <END> to the start and end of each sentence \n",
        "        tmp = line.split()\n",
        "        for word in tmp:\n",
        "            if word in train.keys():            # if in dict, increment counter\n",
        "                train[word] += 1\n",
        "            else:\n",
        "                train[word] = 1                 # if not in dict, add new entry\n",
        "\n",
        "    countUNK = 0\n",
        "    for key in list(train.keys()):              # find all words with freq < 3, sum their counts\n",
        "        if train[key] < 3:\n",
        "            countUNK += train[key]\n",
        "            del train[key]\n",
        "    train['<UNK>'] = countUNK\n",
        "    print(\"Size of vocabulary = \" + str(len(train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "Size of vocabulary = 26601\n",
        "\n",
        "Number of parameters of n-gram models = (size of vocabulary) ^ n = (26601) ^ n\n",
        "eg unigram: 26601 ^ 1\n",
        "eg bigram: 26601 ^ 2\n",
        "eg trigram: 26601 ^ 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "## 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ACSfNZGE8Yw2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "72045\n",
            "0.04293396708783213\n",
            "1678042\n",
            "26601\n",
            "9\n",
            "5.3633937648759684e-06\n",
            "Unigram probability of 'the' = 0.04293396708783213\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "unigramDict = dict()\n",
        "bigramDict = dict()\n",
        "\n",
        "# build unigram\n",
        "def unigram():\n",
        "    for key in train.keys():\n",
        "        unigramDict[key] = float(train[key]) / sum(train.values())\n",
        "\n",
        "def unigramProb(word):\n",
        "    if word in unigramDict.keys():\n",
        "        return unigramDict[word]\n",
        "    else:\n",
        "        return unigramDict['<UNK>']\n",
        "\n",
        "# build bigram\n",
        "def bigram():\n",
        "    with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            for i in range(len(tmp) - 1):\n",
        "                if tmp[i] not in train.keys():\n",
        "                    tmp[i] = '<UNK>'\n",
        "                if tmp[i+1] not in train.keys():\n",
        "                    tmp[i+1] = '<UNK>'\n",
        "                    \n",
        "                if (tmp[i], tmp[i+1]) in bigramDict.keys():\n",
        "                    bigramDict[(tmp[i], tmp[i+1])] += 1\n",
        "                else:\n",
        "                    bigramDict[(tmp[i], tmp[i+1])] = 1\n",
        "    for key in bigramDict.keys():\n",
        "        # bigramDict[key] = float(bigramDict[key])     # P(w2|w1) = C(w1, w2) / C(w1)\n",
        "        bigramDict[key] = float(bigramDict[key]) / train[key[0]]    # P(w2|w1) = C(w1, w2) / C(w1)\n",
        "        # bigramDict[key] = train[key[0]]    # P(w2|w1) = C(w1, w2) / C(w1)\n",
        "                    \n",
        "unigram()\n",
        "# bigram()\n",
        "print(train['the'])\n",
        "print(unigramDict['the'])\n",
        "print(sum(train.values()))\n",
        "print(str(len(train)))\n",
        "print(train['exceeding'])\n",
        "print(unigramDict['exceeding'])\n",
        "print(\"Unigram probability of 'the' = \" + str(unigramDict['the']))\n",
        "# print(\"Bigram probability of 'the' given 'the' = \" + str(bigramDict[('the', 'the')]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<START>', 'Newquay', \"'s\", 'senior', 'police', 'officer', ',', 'Insp', 'Dave', 'Meredith', ',', 'said', 'the', 'application', 'would', 'be', 'looked', 'at', 'closely', '.', '<END>']\n",
            "-3.774842084590672 0.0365282871346486 <START> <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-22.45318947960522 2.383730562167097e-06 Newquay <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-29.38166744722293 0.008209568056103482 's <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-42.12337690323491 0.0001460034969327347 senior <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-52.93444555853981 0.0005566010862660172 police <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-65.96174126237543 0.00011978246074889664 officer <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-70.52604135064333 0.042267714395706424 , <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-87.11692590440754 1.0130854889210162e-05 Insp <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-102.93729230429452 1.7282046575711453e-05 Dave <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-119.91519998116797 7.747124327043066e-06 Meredith <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-124.47950006943587 0.042267714395706424 , <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-132.05767793332052 0.00523288451659732 said <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-136.59941463948914 0.04293396708783213 the <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-151.5773223163626 3.0988497308172265e-05 application <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-160.67211678091067 0.0018289172738227052 would <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-168.74128543778323 0.0037233871381050056 be <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-182.21996048796143 8.760209815964082e-05 looked <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-190.0741446732318 0.004321703509208947 at <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-204.430563973359 4.767461124334194e-05 closely <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-209.2191697639131 0.03618145433785328 . <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n",
            "-213.99401184850376 0.0365282871346486 <END> <START> Newquay 's senior police officer , Insp Dave Meredith , said the application would be looked at closely .\n",
            " <END>\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "math domain error",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\grace\\OneDrive\\桌面\\comp3361-asm\\A1\\A1.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y243sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         perplexity \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mpow(perplexity, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39mfloat\u001b[39m(N))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y243sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m perplexity\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y243sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUnigram train perplexity = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(unigramTrainPerplexity()))\n",
            "\u001b[1;32mc:\\Users\\grace\\OneDrive\\桌面\\comp3361-asm\\A1\\A1.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y243sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m             prob \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mlog2(unigramProb(word))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y243sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m             \u001b[39mprint\u001b[39m(prob, unigramProb(word), word, line)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y243sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39msum\u001b[39m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m math\u001b[39m.\u001b[39;49mlog2(prob)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y243sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (\u001b[39m-\u001b[39m\u001b[39msum\u001b[39m \u001b[39m/\u001b[39m N)\n",
            "\u001b[1;31mValueError\u001b[0m: math domain error"
          ]
        }
      ],
      "source": [
        "def unigramTrainPerplexity():\n",
        "    with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        N = 0\n",
        "        sum = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            print(tmp)\n",
        "            prob = 1\n",
        "            for word in tmp:\n",
        "                N += 1\n",
        "                prob += math.log2(unigramProb(word))\n",
        "                print(prob, unigramProb(word), word, line)\n",
        "            sum += math.log2(prob)\n",
        "    return 2 ** (-sum / N)\n",
        "    \n",
        "def unigramDevPerplexity():\n",
        "    with open('data/lm/dev.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        perplexity = 1\n",
        "        N = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            for word in tmp:\n",
        "                N += 1\n",
        "                perplexity *= 1 / unigramProb(word)\n",
        "        perplexity = math.pow(perplexity, 1/float(N))\n",
        "        return perplexity\n",
        "\n",
        "def bigramTrainPerplexity():\n",
        "    with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        perplexity = 1\n",
        "        N = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            for i in range(len(tmp) - 1):\n",
        "                N += 1\n",
        "                if (tmp[i], tmp[i+1]) in bigramDict.keys():\n",
        "                    perplexity *= 1 / bigramDict[(tmp[i], tmp[i+1])]\n",
        "                else:\n",
        "                    perplexity *= 1 / unigramProb(tmp[i+1])\n",
        "        perplexity = math.pow(perplexity, 1/float(N))\n",
        "        return perplexity\n",
        "    \n",
        "def bigramDevPerplexity():\n",
        "    with open('data/lm/dev.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        perplexity = 1\n",
        "        N = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            for i in range(len(tmp) - 1):\n",
        "                N += 1\n",
        "                if (tmp[i], tmp[i+1]) in bigramDict.keys():\n",
        "                    perplexity *= 1 / bigramDict[(tmp[i], tmp[i+1])]\n",
        "                else:\n",
        "                    perplexity *= 1 / unigramProb(tmp[i+1])\n",
        "        perplexity = math.pow(perplexity, 1/float(N))\n",
        "        return perplexity\n",
        "    \n",
        "print(\"Unigram train perplexity = \" + str(unigramTrainPerplexity()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4gcgL--Ylh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "## 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "### 1.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93_yLu9dVr0C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "#### 1.3.2 Add-k smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhcuJWo7VuNg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOUpNXYVuNh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "### 1.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4N_XuN6WVCQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkf6IV0WVCQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "##### 1.3.4 Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 252.1/252.1MB downloaded\n",
            "Loaded vocab size 400000\n"
          ]
        }
      ],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 50\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
        "    return wv_from_bin\n",
        "\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Find most similar word\n",
        "Use cosine similarity to find the most similar word to each of these words. Report the most similar word and its cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer format: {word: [most similar word, cosine similarity]}\n",
            "{'dog': ['dogs', 0.81368625], 'whale': ['whales', 0.7918058], 'before': ['after', 0.8931248], 'however': ['although', 0.9336163], 'fabricate': ['fabricating', 0.61840194]}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "wordList = [\"dog\", \"whale\", \"before\", \"however\", \"fabricate\"]\n",
        "wordDict = {}\n",
        "vecKeys = wv_from_bin.index_to_key\n",
        "for word in wordList:\n",
        "  sim = -1\n",
        "  most = \"\"\n",
        "  for i in range(len(wv_from_bin)):\n",
        "    x = wv_from_bin[word]\n",
        "    cosine = np.dot(x,wv_from_bin[i])/(norm(x)*norm(wv_from_bin[i]))\n",
        "    curword = vecKeys[i]\n",
        "    if cosine >= sim and curword != word:\n",
        "      sim = cosine\n",
        "      most = curword\n",
        "  wordDict[word] = [most, sim]\n",
        "print(\"Answer format: {word: [most similar word, cosine similarity]}\")\n",
        "print(wordDict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Finding Analogies\n",
        "Use vector addition and subtraction to compute target vectors for the analogies below. After computing each target vector, find the top three candidates by cosine similarity. Report the candidates and their similarities to the target vector.\n",
        "\n",
        "- dog : puppy :: cat : ?:\n",
        "- speak : speaker :: sing : ?:\n",
        "- France : French :: England : ?:\n",
        "- France : wine :: England : ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dog : puppy :: cat : ?: ['cat', 'dog', 'dogs']\n",
            "corresponding similarities: [0.8246017, 0.745641, 0.6278181]\n",
            "speak : speaker :: sing : ?: ['sing', 'speak', 'singing']\n",
            "corresponding similarities: [0.7365669, 0.55140996, 0.548521]\n",
            "france : french :: england : ?: ['england', 'wales', 'ireland']\n",
            "corresponding similarities: [0.837254, 0.6498687, 0.6262423]\n",
            "france : wine :: england : ?: ['england', 'france', 'britain']\n",
            "corresponding similarities: [0.69291466, 0.6897908, 0.61666137]\n"
          ]
        }
      ],
      "source": [
        "# dog-puppy+cat\n",
        "wordList = [[\"dog\", \"puppy\", \"cat\"], [\"speak\", \"speaker\", \"sing\"], [\"france\", \"french\", \"england\"], [\"france\", \"wine\", \"england\"]]\n",
        "\n",
        "vecKeys = wv_from_bin.index_to_key\n",
        "for analogy in wordList:\n",
        "  targetVec = wv_from_bin[analogy[0]] - wv_from_bin[analogy[1]] + wv_from_bin[analogy[2]]\n",
        "  \n",
        "  candidateList = []\n",
        "  similarityList = []\n",
        "  for i in range(len(wv_from_bin)):\n",
        "    cosine = np.dot(targetVec,wv_from_bin[i])/(norm(targetVec)*norm(wv_from_bin[i]))\n",
        "    curword = vecKeys[i]\n",
        "    \n",
        "    if len(similarityList) < 3 or (len(similarityList) >= 3 and cosine >= similarityList[2]):\n",
        "      similarityList.append(cosine) \n",
        "      similarityList.sort(reverse=True)\n",
        "      candidateList.insert(similarityList.index(cosine), curword)\n",
        "  print(f\"{analogy[0]} : {analogy[1]} :: {analogy[2]} : ?:\", candidateList[0:3])\n",
        "  print(\"corresponding similarities:\", similarityList[0:3])\n",
        "# targetVec = wv_from_bin[\"dog\"] - wv_from_bin[\"puppy\"] + wv_from_bin[\"cat\"]\n",
        "# print(\"kitty\", np.dot(targetVec,wv_from_bin[\"kitty\"])/(norm(targetVec)*norm(wv_from_bin[\"kitty\"])))\n",
        "# print(\"kitten\", np.dot(targetVec,wv_from_bin[\"kitten\"])/(norm(targetVec)*norm(wv_from_bin[\"kitten\"])))\n",
        "# print(\"dogs\", np.dot(targetVec,wv_from_bin[\"dogs\"])/(norm(targetVec)*norm(wv_from_bin[\"dogs\"])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The syntax of the command is incorrect.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/classification\n",
        "!wget -O data/classification/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/train.txt\n",
        "!wget -O data/classification/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/dev.txt\n",
        "!wget -O data/classification/test-blind.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/test-blind.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GloVe Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the performance of three types of features on dev set. Report the weighted average precision, recall and F1-score for each feature set.\n",
        "\n",
        "| Feature | precision | recall | F1-score |\n",
        "| ----------- | --------- | ------ | -------- |\n",
        "| unigram     |           |        |          |\n",
        "| bigram      |           |        |          |\n",
        "| GloVe       |           |        |          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Better Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.12.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
