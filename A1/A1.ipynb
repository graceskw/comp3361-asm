{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.txt** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **1. clean all outputs and 2. run all cells in your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "# 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YsSAtTqt7Q8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The syntax of the command is incorrect.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/lm\n",
        "!wget -O data/lm/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/train.txt\n",
        "!wget -O data/lm/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/dev.txt\n",
        "!wget -O data/lm/test.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "## 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q-rNT_QL8Dvt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary = 26601\n"
          ]
        }
      ],
      "source": [
        "with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    train = dict()\n",
        "    for line in f:\n",
        "        line = '<START> ' + line + ' <END>'     # to add <START> and <END> to the start and end of each sentence \n",
        "        tmp = line.split()\n",
        "        for word in tmp:\n",
        "            if word in train.keys():            # if in dict, increment counter\n",
        "                train[word] += 1\n",
        "            else:\n",
        "                train[word] = 1                 # if not in dict, add new entry\n",
        "\n",
        "    countUNK = 0\n",
        "    for key in list(train.keys()):              # find all words with freq < 3, sum their counts\n",
        "        if train[key] < 3:\n",
        "            countUNK += train[key]\n",
        "            del train[key]\n",
        "    train['<UNK>'] = countUNK\n",
        "    print(\"Size of vocabulary = \" + str(len(train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "Size of vocabulary = 26601\n",
        "\n",
        "Number of parameters of n-gram models = (size of vocabulary) ^ n = (26601) ^ n\n",
        "eg unigram: 26601 ^ 1\n",
        "eg bigram: 26601 ^ 2\n",
        "eg trigram: 26601 ^ 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "## 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "ACSfNZGE8Yw2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "unigramDict = dict()\n",
        "bigramDict = dict()\n",
        "bigramTrain = dict()\n",
        "\n",
        "# build unigram\n",
        "def unigram():\n",
        "    for key in train.keys():\n",
        "        unigramDict[key] = float(train[key]) / sum(train.values())\n",
        "\n",
        "def unigramProb(word):\n",
        "    if word in unigramDict.keys():\n",
        "        return unigramDict[word]\n",
        "    else:\n",
        "        return unigramDict['<UNK>']\n",
        "\n",
        "# build bigram\n",
        "def bigramVocab():\n",
        "    with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            for i in range(len(tmp) - 1):\n",
        "                if tmp[i] not in train.keys():\n",
        "                    tmp[i] = '<UNK>'\n",
        "                if tmp[i+1] not in train.keys():\n",
        "                    tmp[i+1] = '<UNK>'\n",
        "                    \n",
        "                if (tmp[i], tmp[i+1]) in bigramTrain.keys():\n",
        "                    bigramTrain[(tmp[i], tmp[i+1])] += 1\n",
        "                else:\n",
        "                    bigramTrain[(tmp[i], tmp[i+1])] = 1\n",
        "\n",
        "def bigram():\n",
        "    for key in bigramTrain.keys():\n",
        "        bigramDict[key] = float(bigramTrain[key]) / train[key[0]]    # P(w2|w1) = C(w1, w2) / C(w1)\n",
        "           \n",
        "unigram()\n",
        "bigramVocab()\n",
        "bigram()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram train perplexity = 866.6059171242847\n",
            "Unigram dev perplexity = 795.3775132279369\n",
            "Bigram train perplexity = 74.93908934229735\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "math domain error",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\grace\\OneDrive\\桌面\\comp3361-asm\\A1\\A1.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y114sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUnigram dev perplexity = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(unigramPerplexity(\u001b[39m\"\u001b[39m\u001b[39mdev\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y114sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBigram train perplexity = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(bigramPerplexity(\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y114sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBigram dev perplexity = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(bigramPerplexity(\u001b[39m\"\u001b[39;49m\u001b[39mdev\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n",
            "\u001b[1;32mc:\\Users\\grace\\OneDrive\\桌面\\comp3361-asm\\A1\\A1.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y114sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                 prob \u001b[39m=\u001b[39m bigramDict[(tmp[i], tmp[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y114sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m             \u001b[39m# logProb += math.log2(bigramDict[(tmp[i], tmp[i+1])])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y114sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             logProb \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m math\u001b[39m.\u001b[39;49mlog2(prob)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y114sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39msum\u001b[39m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (logProb)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/OneDrive/%E6%A1%8C%E9%9D%A2/comp3361-asm/A1/A1.ipynb#Y114sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (\u001b[39m-\u001b[39m\u001b[39msum\u001b[39m \u001b[39m/\u001b[39m N)\n",
            "\u001b[1;31mValueError\u001b[0m: math domain error"
          ]
        }
      ],
      "source": [
        "def unigramPerplexity(set):\n",
        "    with open(f'data/lm/{set}.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        N = 0\n",
        "        sum = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            # print(tmp)\n",
        "            logProb = 1\n",
        "            for word in tmp:\n",
        "                N += 1\n",
        "                # logProb *= unigramProb(word)\n",
        "                logProb += math.log2(unigramProb(word))\n",
        "                # print(logProb, unigramProb(word), word, line)\n",
        "            sum += (logProb)\n",
        "    return 2 ** (-sum / N)\n",
        "\n",
        "def bigramPerplexity(set):\n",
        "    with open(f'data/lm/{set}.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        N = 0\n",
        "        sum = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            logProb = 1\n",
        "            for i in range(len(tmp) - 1):\n",
        "                N += 1\n",
        "                if tmp[i] not in train.keys():\n",
        "                    tmp[i] = '<UNK>'\n",
        "                if tmp[i+1] not in train.keys():\n",
        "                    tmp[i+1] = '<UNK>'\n",
        "                    \n",
        "                if (tmp[i], tmp[i+1]) not in bigramDict.keys():\n",
        "                    prob = 0 / train[key[0]]\n",
        "                else:\n",
        "                    prob = bigramDict[(tmp[i], tmp[i+1])]\n",
        "                # logProb += math.log2(bigramDict[(tmp[i], tmp[i+1])])\n",
        "                logProb += math.log2(prob)\n",
        "            sum += (logProb)\n",
        "    return 2 ** (-sum / N)\n",
        "\n",
        "print(\"Unigram train perplexity = \" + str(unigramPerplexity(\"train\")))\n",
        "print(\"Unigram dev perplexity = \" + str(unigramPerplexity(\"dev\")))\n",
        "print(\"Bigram train perplexity = \" + str(bigramPerplexity(\"train\")))\n",
        "print(\"Bigram dev perplexity = \" + str(bigramPerplexity(\"dev\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4gcgL--Ylh"
      },
      "source": [
        "When calculating the perplexity of unigram on the dev set, even if the words aren't in the training set, they are counted as \\<UNK\\>, a valid token in the vocabulary.\n",
        "However, when it comes to the bigram model, some new combinations of words are in the dev set does not exist in the training set. These combinations have a count and probability of zero, making the log likelihood of the model negative infinity which caused error when calculating the perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "## 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "### 1.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "93_yLu9dVr0C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram train perplexity = 1405.177331370177\n",
            "Bigram dev perplexity = 1580.6409074084834\n"
          ]
        }
      ],
      "source": [
        "bigramTrainDict = dict()\n",
        "bigramSmoothDict = dict()\n",
        "\n",
        "def bigramTrain():\n",
        "    with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            for i in range(len(tmp) - 1):\n",
        "                if tmp[i] not in train.keys():\n",
        "                    tmp[i] = '<UNK>'\n",
        "                if tmp[i+1] not in train.keys():\n",
        "                    tmp[i+1] = '<UNK>'\n",
        "                    \n",
        "                if (tmp[i], tmp[i+1]) in bigramTrainDict.keys():\n",
        "                    bigramTrainDict[(tmp[i], tmp[i+1])] += 1\n",
        "                else:\n",
        "                    bigramTrainDict[(tmp[i], tmp[i+1])] = 1\n",
        "\n",
        "def bigramSmoothPerplexity(set, k):\n",
        "    for key in bigramTrainDict.keys():\n",
        "        bigramSmoothDict[key] = (float(bigramTrainDict[key]) + k) / (train[key[0]] + k*26601)    # P(w2|w1) = (C(w1, w2) + 1)/ (C(w1) + |V|)\n",
        "    \n",
        "    with open(f'data/lm/{set}.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        N = 0\n",
        "        sum = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            logProb = 1\n",
        "            for i in range(len(tmp) - 1):\n",
        "                N += 1\n",
        "                if tmp[i] not in train.keys():\n",
        "                    tmp[i] = '<UNK>'\n",
        "                if tmp[i+1] not in train.keys():\n",
        "                    tmp[i+1] = '<UNK>'\n",
        "\n",
        "                if (tmp[i], tmp[i+1]) not in bigramSmoothDict.keys():\n",
        "                    prob = (k) / (train[key[0]] + k*26601)\n",
        "                else:\n",
        "                    prob = bigramSmoothDict[(tmp[i], tmp[i+1])]\n",
        "                    \n",
        "                logProb += math.log2(prob)\n",
        "            sum += (logProb)\n",
        "    return 2 ** (-sum / N)\n",
        "\n",
        "bigramTrain()\n",
        "print(\"Bigram train perplexity = \" + str(bigramSmoothPerplexity(\"train\", 1)))\n",
        "print(\"Bigram dev perplexity = \" + str(bigramSmoothPerplexity(\"dev\", 1)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "#### 1.3.2 Add-k smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "jhcuJWo7VuNg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram train perplexity (k=0.001) = 92.49453763806177\n",
            "Bigram train perplexity (k=0.01) = 153.61489113220662\n",
            "Bigram train perplexity (k=0.1) = 396.8822034902569\n",
            "\n",
            "Bigram dev perplexity (k=0.001) = 432.7520905911309\n",
            "Bigram dev perplexity (k=0.01) = 395.3147101966442\n",
            "Bigram dev perplexity (k=0.1) = 631.6455075022197\n"
          ]
        }
      ],
      "source": [
        "print(\"Bigram train perplexity (k=0.001) = \" + str(bigramSmoothPerplexity(\"train\", 0.001)))\n",
        "print(\"Bigram train perplexity (k=0.01) = \" + str(bigramSmoothPerplexity(\"train\", 0.01)))\n",
        "print(\"Bigram train perplexity (k=0.1) = \" + str(bigramSmoothPerplexity(\"train\", 0.1)))\n",
        "print()\n",
        "print(\"Bigram dev perplexity (k=0.001) = \" + str(bigramSmoothPerplexity(\"dev\", 0.001)))\n",
        "print(\"Bigram dev perplexity (k=0.01) = \" + str(bigramSmoothPerplexity(\"dev\", 0.01)))\n",
        "print(\"Bigram dev perplexity (k=0.1) = \" + str(bigramSmoothPerplexity(\"dev\", 0.1)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOUpNXYVuNh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "### 1.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4N_XuN6WVCQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkf6IV0WVCQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "##### 1.3.4 Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vocab size 400000\n"
          ]
        }
      ],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 50\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
        "    return wv_from_bin\n",
        "\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Find most similar word\n",
        "Use cosine similarity to find the most similar word to each of these words. Report the most similar word and its cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer format: {word: [most similar word, cosine similarity]}\n",
            "{'dog': ['dogs', 0.81368625], 'whale': ['whales', 0.7918058], 'before': ['after', 0.8931248], 'however': ['although', 0.9336163], 'fabricate': ['fabricating', 0.61840194]}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "wordList = [\"dog\", \"whale\", \"before\", \"however\", \"fabricate\"]\n",
        "wordDict = {}\n",
        "vecKeys = wv_from_bin.index_to_key\n",
        "for word in wordList:\n",
        "  sim = -1\n",
        "  most = \"\"\n",
        "  for i in range(len(wv_from_bin)):\n",
        "    x = wv_from_bin[word]\n",
        "    cosine = np.dot(x,wv_from_bin[i])/(norm(x)*norm(wv_from_bin[i]))\n",
        "    curword = vecKeys[i]\n",
        "    if cosine >= sim and curword != word:\n",
        "      sim = cosine\n",
        "      most = curword\n",
        "  wordDict[word] = [most, sim]\n",
        "print(\"Answer format: {word: [most similar word, cosine similarity]}\")\n",
        "print(wordDict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Finding Analogies\n",
        "Use vector addition and subtraction to compute target vectors for the analogies below. After computing each target vector, find the top three candidates by cosine similarity. Report the candidates and their similarities to the target vector.\n",
        "\n",
        "- dog : puppy :: cat : ?:\n",
        "- speak : speaker :: sing : ?:\n",
        "- France : French :: England : ?:\n",
        "- France : wine :: England : ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dog : puppy :: cat : ?: ['cat', 'dog', 'dogs']\n",
            "corresponding similarities: [0.8246017, 0.745641, 0.6278181]\n",
            "speak : speaker :: sing : ?: ['sing', 'speak', 'singing']\n",
            "corresponding similarities: [0.7365669, 0.55140996, 0.548521]\n",
            "france : french :: england : ?: ['england', 'wales', 'ireland']\n",
            "corresponding similarities: [0.837254, 0.6498687, 0.6262423]\n",
            "france : wine :: england : ?: ['england', 'france', 'britain']\n",
            "corresponding similarities: [0.69291466, 0.6897908, 0.61666137]\n"
          ]
        }
      ],
      "source": [
        "# dog-puppy+cat\n",
        "wordList = [[\"dog\", \"puppy\", \"cat\"], [\"speak\", \"speaker\", \"sing\"], [\"france\", \"french\", \"england\"], [\"france\", \"wine\", \"england\"]]\n",
        "\n",
        "vecKeys = wv_from_bin.index_to_key\n",
        "for analogy in wordList:\n",
        "  targetVec = wv_from_bin[analogy[0]] - wv_from_bin[analogy[1]] + wv_from_bin[analogy[2]]\n",
        "  \n",
        "  candidateList = []\n",
        "  similarityList = []\n",
        "  for i in range(len(wv_from_bin)):\n",
        "    cosine = np.dot(targetVec,wv_from_bin[i])/(norm(targetVec)*norm(wv_from_bin[i]))\n",
        "    curword = vecKeys[i]\n",
        "    \n",
        "    if len(similarityList) < 3 or (len(similarityList) >= 3 and cosine >= similarityList[2]):\n",
        "      similarityList.append(cosine) \n",
        "      similarityList.sort(reverse=True)\n",
        "      candidateList.insert(similarityList.index(cosine), curword)\n",
        "  print(f\"{analogy[0]} : {analogy[1]} :: {analogy[2]} : ?:\", candidateList[0:3])\n",
        "  print(\"corresponding similarities:\", similarityList[0:3])\n",
        "# targetVec = wv_from_bin[\"dog\"] - wv_from_bin[\"puppy\"] + wv_from_bin[\"cat\"]\n",
        "# print(\"kitty\", np.dot(targetVec,wv_from_bin[\"kitty\"])/(norm(targetVec)*norm(wv_from_bin[\"kitty\"])))\n",
        "# print(\"kitten\", np.dot(targetVec,wv_from_bin[\"kitten\"])/(norm(targetVec)*norm(wv_from_bin[\"kitten\"])))\n",
        "# print(\"dogs\", np.dot(targetVec,wv_from_bin[\"dogs\"])/(norm(targetVec)*norm(wv_from_bin[\"dogs\"])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The syntax of the command is incorrect.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/classification\n",
        "!wget -O data/classification/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/train.txt\n",
        "!wget -O data/classification/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/dev.txt\n",
        "!wget -O data/classification/test-blind.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/test-blind.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GloVe Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the performance of three types of features on dev set. Report the weighted average precision, recall and F1-score for each feature set.\n",
        "\n",
        "| Feature | precision | recall | F1-score |\n",
        "| ----------- | --------- | ------ | -------- |\n",
        "| unigram     |           |        |          |\n",
        "| bigram      |           |        |          |\n",
        "| GloVe       |           |        |          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Better Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.12.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
