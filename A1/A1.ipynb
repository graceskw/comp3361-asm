{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.txt** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **1. clean all outputs and 2. run all cells in your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "# 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YsSAtTqt7Q8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The syntax of the command is incorrect.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/lm\n",
        "!wget -O data/lm/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/train.txt\n",
        "!wget -O data/lm/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/dev.txt\n",
        "!wget -O data/lm/test.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "## 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "q-rNT_QL8Dvt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary = 26601\n"
          ]
        }
      ],
      "source": [
        "with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    train = dict()\n",
        "    for line in f:\n",
        "        line = '<START> ' + line + ' <END>'     # to add <START> and <END> to the start and end of each sentence \n",
        "        tmp = line.split()\n",
        "        for word in tmp:\n",
        "            if word in train.keys():            # if in dict, increment counter\n",
        "                train[word] += 1\n",
        "            else:\n",
        "                train[word] = 1                 # if not in dict, add new entry\n",
        "\n",
        "    countUNK = 0\n",
        "    for key in list(train.keys()):              # find all words with freq < 3, sum their counts\n",
        "        if train[key] < 3:\n",
        "            countUNK += train[key]\n",
        "            del train[key]\n",
        "    train['<UNK>'] = countUNK\n",
        "    print(\"Size of vocabulary = \" + str(len(train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "Size of vocabulary = 26601\n",
        "\n",
        "Number of parameters of n-gram models = (size of vocabulary) ^ n = (26601) ^ n\n",
        "eg unigram: 26601 ^ 1\n",
        "eg bigram: 26601 ^ 2\n",
        "eg trigram: 26601 ^ 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "## 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ACSfNZGE8Yw2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "unigramDict = dict()\n",
        "bigramDict = dict()\n",
        "bigramTrain = dict()\n",
        "\n",
        "# build unigram\n",
        "def unigram():\n",
        "    for key in train.keys():\n",
        "        unigramDict[key] = float(train[key]) / sum(train.values())\n",
        "\n",
        "def unigramProb(word):\n",
        "    if word in unigramDict.keys():\n",
        "        return unigramDict[word]\n",
        "    else:\n",
        "        return unigramDict['<UNK>']\n",
        "\n",
        "# build bigram\n",
        "def bigramVocab():\n",
        "    with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            for i in range(len(tmp) - 1):\n",
        "                if tmp[i] not in train.keys():\n",
        "                    tmp[i] = '<UNK>'\n",
        "                if tmp[i+1] not in train.keys():\n",
        "                    tmp[i+1] = '<UNK>'\n",
        "                    \n",
        "                if (tmp[i], tmp[i+1]) in bigramTrain.keys():\n",
        "                    bigramTrain[(tmp[i], tmp[i+1])] += 1\n",
        "                else:\n",
        "                    bigramTrain[(tmp[i], tmp[i+1])] = 1\n",
        "\n",
        "def bigram():\n",
        "    for key in bigramTrain.keys():\n",
        "        bigramDict[key] = float(bigramTrain[key]) / train[key[0]]    # P(w2|w1) = C(w1, w2) / C(w1)\n",
        "           \n",
        "unigram()\n",
        "bigramVocab()\n",
        "bigram()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram train perplexity = 888.8280671219554\n",
            "Unigram dev perplexity = 815.9305409758995\n",
            "Bigram train perplexity = 76.93455019701061\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "math domain error",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\grace\\Desktop\\comp3361-asm\\A1\\A1.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUnigram dev perplexity = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(unigramPerplexity(\u001b[39m\"\u001b[39m\u001b[39mdev\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X14sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBigram train perplexity = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(bigramPerplexity(\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBigram dev perplexity = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(bigramPerplexity(\u001b[39m\"\u001b[39;49m\u001b[39mdev\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n",
            "\u001b[1;32mc:\\Users\\grace\\Desktop\\comp3361-asm\\A1\\A1.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                 prob \u001b[39m=\u001b[39m bigramDict[(tmp[i], tmp[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m             \u001b[39m# logProb += math.log2(bigramDict[(tmp[i], tmp[i+1])])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             logProb \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m math\u001b[39m.\u001b[39;49mlog2(prob)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39msum\u001b[39m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (logProb)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (\u001b[39m-\u001b[39m\u001b[39msum\u001b[39m \u001b[39m/\u001b[39m N)\n",
            "\u001b[1;31mValueError\u001b[0m: math domain error"
          ]
        }
      ],
      "source": [
        "def unigramPerplexity(set):\n",
        "    with open(f'data/lm/{set}.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        N = 0\n",
        "        sum = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            # print(tmp)\n",
        "            logProb = 0\n",
        "            for word in tmp:\n",
        "                N += 1\n",
        "                # logProb *= unigramProb(word)\n",
        "                logProb += math.log2(unigramProb(word))\n",
        "                # print(logProb, unigramProb(word), word, line)\n",
        "            sum += (logProb)\n",
        "    return 2 ** (-sum / N)\n",
        "\n",
        "def bigramPerplexity(set):\n",
        "    with open(f'data/lm/{set}.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        N = 0\n",
        "        sum = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            logProb = 0\n",
        "            for i in range(len(tmp) - 1):\n",
        "                N += 1\n",
        "                if tmp[i] not in train.keys():\n",
        "                    tmp[i] = '<UNK>'\n",
        "                if tmp[i+1] not in train.keys():\n",
        "                    tmp[i+1] = '<UNK>'\n",
        "                    \n",
        "                if (tmp[i], tmp[i+1]) not in bigramDict.keys():\n",
        "                    prob = 0 / train[tmp[i]]\n",
        "                else:\n",
        "                    prob = bigramDict[(tmp[i], tmp[i+1])]\n",
        "                # logProb += math.log2(bigramDict[(tmp[i], tmp[i+1])])\n",
        "                logProb += math.log2(prob)\n",
        "            sum += (logProb)\n",
        "    return 2 ** (-sum / N)\n",
        "\n",
        "print(\"Unigram train perplexity = \" + str(unigramPerplexity(\"train\")))\n",
        "print(\"Unigram dev perplexity = \" + str(unigramPerplexity(\"dev\")))\n",
        "print(\"Bigram train perplexity = \" + str(bigramPerplexity(\"train\")))\n",
        "print(\"Bigram dev perplexity = \" + str(bigramPerplexity(\"dev\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4gcgL--Ylh"
      },
      "source": [
        "When calculating the perplexity of unigram on the dev set, even if the words aren't in the training set, they are counted as \\<UNK\\>, a valid token in the vocabulary.\n",
        "However, when it comes to the bigram model, some new combinations of words are in the dev set does not exist in the training set. These combinations have a count and probability of zero, making the log likelihood of the model negative infinity which caused error when calculating the perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "## 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "### 1.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "93_yLu9dVr0C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram train perplexity = 1442.5940705284504\n",
            "Bigram dev perplexity = 1660.2006059675152\n"
          ]
        }
      ],
      "source": [
        "bigramSmoothDict = dict()\n",
        "\n",
        "def bigramSmoothPerplexity(set, k):\n",
        "    for key in bigramTrain.keys():\n",
        "        bigramSmoothDict[key] = (float(bigramTrain[key]) + k) / (train[key[0]] + k*26601)    # P(w2|w1) = (C(w1, w2) + 1)/ (C(w1) + |V|)\n",
        "    \n",
        "    with open(f'data/lm/{set}.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        N = 0\n",
        "        sum = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            logProb = 0\n",
        "            for i in range(len(tmp) - 1):\n",
        "                N += 1\n",
        "                if tmp[i] not in train.keys():\n",
        "                    tmp[i] = '<UNK>'\n",
        "                if tmp[i+1] not in train.keys():\n",
        "                    tmp[i+1] = '<UNK>'\n",
        "\n",
        "                if (tmp[i], tmp[i+1]) not in bigramSmoothDict.keys():\n",
        "                    prob = (k) / (train[tmp[i]] + k*26601)\n",
        "                else:\n",
        "                    prob = bigramSmoothDict[(tmp[i], tmp[i+1])]\n",
        "                    \n",
        "                logProb += math.log2(prob)\n",
        "            sum += (logProb)\n",
        "    return 2 ** (-sum / N)\n",
        "\n",
        "# bigramTrain()\n",
        "print(\"Bigram train perplexity = \" + str(bigramSmoothPerplexity(\"train\", 1)))\n",
        "print(\"Bigram dev perplexity = \" + str(bigramSmoothPerplexity(\"dev\", 1)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": [
        "The perplexity after smoothing is worse than before smoothing in 1.2. This is caused by the nature of add-one smoothing being overly simplistic, shifting too much probability mass from seen to unseen bigrams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "#### 1.3.2 Add-k smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jhcuJWo7VuNg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram train perplexity (k=0.001) = 94.95746093685625\n",
            "Bigram train perplexity (k=0.01) = 157.7053131622282\n",
            "Bigram train perplexity (k=0.1) = 407.4502916262101\n",
            "\n",
            "Bigram dev perplexity (k=0.001) = 432.4287195649327\n",
            "Bigram dev perplexity (k=0.01) = 437.56977580286156\n",
            "Bigram dev perplexity (k=0.1) = 695.6444386896961\n"
          ]
        }
      ],
      "source": [
        "print(\"Bigram train perplexity (k=0.001) = \" + str(bigramSmoothPerplexity(\"train\", 0.001)))\n",
        "print(\"Bigram train perplexity (k=0.01) = \" + str(bigramSmoothPerplexity(\"train\", 0.01)))\n",
        "print(\"Bigram train perplexity (k=0.1) = \" + str(bigramSmoothPerplexity(\"train\", 0.1)))\n",
        "print()\n",
        "print(\"Bigram dev perplexity (k=0.001) = \" + str(bigramSmoothPerplexity(\"dev\", 0.001)))\n",
        "print(\"Bigram dev perplexity (k=0.01) = \" + str(bigramSmoothPerplexity(\"dev\", 0.01)))\n",
        "print(\"Bigram dev perplexity (k=0.1) = \" + str(bigramSmoothPerplexity(\"dev\", 0.1)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOUpNXYVuNh"
      },
      "source": [
        "From my result, it seems that the perplexity is lower when k is smaller. When k gets closer to 0, the calculated perplexity is also closer to the perplexity of bigram model without any smoothing, while taking care of unseen bigrams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "### 1.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "K4N_XuN6WVCQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear interpolation train perplexity (l1=0.2, l2=0.3, l3=0.5) = 179.4409002188491\n",
            "Linear interpolation dev perplexity (l1=0.2, l2=0.3, l3=0.5) = 403.89796582139195\n"
          ]
        }
      ],
      "source": [
        "trigramDict = dict()\n",
        "trigramTrain = dict()\n",
        "def trigram():\n",
        "    with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = '<START> <START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            for i in range(len(tmp) - 2):\n",
        "                if (tmp[i], tmp[i-1], tmp[i+2]) in trigramTrain.keys():\n",
        "                    trigramTrain[(tmp[i], tmp[i+1], tmp[i+2])] += 1\n",
        "                else:\n",
        "                    trigramTrain[(tmp[i], tmp[i+1], tmp[i+2])] = 1\n",
        "\n",
        "    for key in trigramTrain.keys():\n",
        "        trigramDict[key] = float(trigramTrain[key]) / bigramDict[(key[0], key[1])]    # P(w3|w1, w2) = C(w1, w2, w3) / C(w1, w2)\n",
        "\n",
        "# l1 * P(w3|w1, w2) + l2 * P(w2|w1) + l3 * P(w1)\n",
        "def linearPerplexity(set, l1, l2, l3):\n",
        "    with open(f'data/lm/{set}.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        N = 0\n",
        "        sum = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            logProb = 0\n",
        "            for i in range(2, len(tmp)):\n",
        "                N += 1\n",
        "                if tmp[i] not in train.keys():\n",
        "                    tmp[i] = '<UNK>'\n",
        "                if tmp[i-1] not in train.keys():\n",
        "                    tmp[i-1] = '<UNK>'\n",
        "                if tmp[i-2] not in train.keys():\n",
        "                    tmp[i-2] = '<UNK>'\n",
        "                    \n",
        "                # trigram: l1 * P(w3|w1, w2)\n",
        "                if (tmp[i-2], tmp[i-1], tmp[i]) not in trigramDict.keys():\n",
        "                    prob = 0\n",
        "                else:\n",
        "                    prob = l1 * trigramDict[(tmp[i-2], tmp[i-1], tmp[i])]\n",
        "\n",
        "                # bigram: l2 * P(w2|w1)\n",
        "                if (tmp[i-1], tmp[i]) not in bigramDict.keys():\n",
        "                    prob +=  0\n",
        "                else:\n",
        "                    prob += l2 * bigramDict[(tmp[i-1], tmp[i])]\n",
        "                \n",
        "                # unigram: l3 * P(w1)\n",
        "                prob += l3 * unigramProb(tmp[i]) \n",
        "                \n",
        "                logProb += math.log2(prob)\n",
        "\n",
        "            sum += (logProb)\n",
        "    return 2 ** (-sum / N)\n",
        "\n",
        "print(\"Linear interpolation train perplexity (l1=0.2, l2=0.3, l3=0.5) = \" + str(linearPerplexity(\"train\", 0.2, 0.3, 0.5)))\n",
        "print(\"Linear interpolation dev perplexity (l1=0.2, l2=0.3, l3=0.5) = \" + str(linearPerplexity(\"dev\", 0.2, 0.3, 0.5)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear interpolation dev perplexity (l1=0.0, l2=0.0, l3=1.0) = 902.7562574324129\n",
            "Linear interpolation dev perplexity (l1=0.0, l2=0.1, l3=0.9) = 487.83307571311514\n",
            "Linear interpolation dev perplexity (l1=0.0, l2=0.2, l3=0.8) = 395.8292004906807\n",
            "Linear interpolation dev perplexity (l1=0.0, l2=0.3, l3=0.7) = 346.8932206204395\n",
            "Linear interpolation dev perplexity (l1=0.0, l2=0.4, l3=0.6) = 316.8297990611647\n",
            "Linear interpolation dev perplexity (l1=0.0, l2=0.5, l3=0.5) = 297.85260261819815\n",
            "Linear interpolation dev perplexity (l1=0.0, l2=0.6, l3=0.4) = 286.9845930678361\n",
            "Linear interpolation dev perplexity (l1=0.0, l2=0.7, l3=0.3) = 283.7119180501348\n",
            "Linear interpolation dev perplexity (l1=0.0, l2=0.8, l3=0.2) = 290.39149030698076\n",
            "Linear interpolation dev perplexity (l1=0.0, l2=0.9, l3=0.1) = 318.7553419510995\n",
            "Linear interpolation dev perplexity (l1=0.1, l2=0.0, l3=0.9) = 1003.0625082582239\n",
            "Linear interpolation dev perplexity (l1=0.1, l2=0.1, l3=0.8) = 526.2226609557068\n",
            "Linear interpolation dev perplexity (l1=0.1, l2=0.2, l3=0.7) = 425.08062249785945\n",
            "Linear interpolation dev perplexity (l1=0.1, l2=0.3, l3=0.6) = 372.56916726733766\n",
            "Linear interpolation dev perplexity (l1=0.1, l2=0.4, l3=0.5) = 341.41331763881834\n",
            "Linear interpolation dev perplexity (l1=0.1, l2=0.5, l3=0.4) = 323.21853436492614\n",
            "Linear interpolation dev perplexity (l1=0.1, l2=0.6, l3=0.3) = 315.43943710644817\n",
            "Linear interpolation dev perplexity (l1=0.1, l2=0.7, l3=0.2) = 319.7084452178534\n",
            "Linear interpolation dev perplexity (l1=0.1, l2=0.8, l3=0.1) = 348.26184292886006\n",
            "Linear interpolation dev perplexity (l1=0.2, l2=0.0, l3=0.8) = 1128.4453217904725\n",
            "Linear interpolation dev perplexity (l1=0.2, l2=0.1, l3=0.7) = 572.1458138875623\n",
            "Linear interpolation dev perplexity (l1=0.2, l2=0.2, l3=0.6) = 460.19886003161565\n",
            "Linear interpolation dev perplexity (l1=0.2, l2=0.3, l3=0.5) = 403.89796582139195\n",
            "Linear interpolation dev perplexity (l1=0.2, l2=0.4, l3=0.4) = 372.3157532727484\n",
            "Linear interpolation dev perplexity (l1=0.2, l2=0.5, l3=0.3) = 356.7816715342535\n",
            "Linear interpolation dev perplexity (l1=0.2, l2=0.6, l3=0.2) = 356.83724287105815\n",
            "Linear interpolation dev perplexity (l1=0.2, l2=0.7, l3=0.1) = 384.83971782079584\n",
            "Linear interpolation dev perplexity (l1=0.3, l2=0.0, l3=0.7) = 1289.6517963320564\n",
            "Linear interpolation dev perplexity (l1=0.3, l2=0.1, l3=0.6) = 628.3098728367368\n",
            "Linear interpolation dev perplexity (l1=0.3, l2=0.2, l3=0.5) = 503.4904953598258\n",
            "Linear interpolation dev perplexity (l1=0.3, l2=0.3, l3=0.4) = 443.5187851279991\n",
            "Linear interpolation dev perplexity (l1=0.3, l2=0.4, l3=0.3) = 413.3329015822376\n",
            "Linear interpolation dev perplexity (l1=0.3, l2=0.5, l3=0.2) = 405.63769964942657\n",
            "Linear interpolation dev perplexity (l1=0.3, l2=0.6, l3=0.1) = 431.5559941567315\n",
            "Linear interpolation dev perplexity (l1=0.4, l2=0.0, l3=0.6) = 1504.5937623873783\n",
            "Linear interpolation dev perplexity (l1=0.4, l2=0.1, l3=0.5) = 699.0184331520986\n",
            "Linear interpolation dev perplexity (l1=0.4, l2=0.2, l3=0.4) = 558.8537509010299\n",
            "Linear interpolation dev perplexity (l1=0.4, l2=0.3, l3=0.3) = 496.42100436366155\n",
            "Linear interpolation dev perplexity (l1=0.4, l2=0.4, l3=0.2) = 473.1591556596786\n",
            "Linear interpolation dev perplexity (l1=0.4, l2=0.5, l3=0.1) = 493.6289806638332\n",
            "Linear interpolation dev perplexity (l1=0.5, l2=0.0, l3=0.5) = 1805.512514864837\n",
            "Linear interpolation dev perplexity (l1=0.5, l2=0.1, l3=0.4) = 791.6584009813663\n",
            "Linear interpolation dev perplexity (l1=0.5, l2=0.2, l3=0.3) = 633.6595981223239\n",
            "Linear interpolation dev perplexity (l1=0.5, l2=0.3, l3=0.2) = 573.9691861356814\n",
            "Linear interpolation dev perplexity (l1=0.5, l2=0.4, l3=0.1) = 580.7829806139536\n",
            "Linear interpolation dev perplexity (l1=0.6, l2=0.0, l3=0.4) = 2256.890643580967\n",
            "Linear interpolation dev perplexity (l1=0.6, l2=0.1, l3=0.3) = 920.3977200632199\n",
            "Linear interpolation dev perplexity (l1=0.6, l2=0.2, l3=0.2) = 744.6315065454803\n",
            "Linear interpolation dev perplexity (l1=0.6, l2=0.3, l3=0.1) = 713.6744857420996\n",
            "Linear interpolation dev perplexity (l1=0.7, l2=0.0, l3=0.3) = 3009.1875247747234\n",
            "Linear interpolation dev perplexity (l1=0.7, l2=0.1, l3=0.2) = 1117.7075018020323\n",
            "Linear interpolation dev perplexity (l1=0.7, l2=0.2, l3=0.1) = 946.3183113193526\n",
            "Linear interpolation dev perplexity (l1=0.8, l2=0.0, l3=0.2) = 4513.781287161963\n",
            "Linear interpolation dev perplexity (l1=0.8, l2=0.1, l3=0.1) = 1489.2630130909533\n",
            "Linear interpolation dev perplexity (l1=0.9, l2=0.0, l3=0.1) = 9027.562574323969\n",
            "Best l1, l2, l3 = 0.0, 0.7, 0.3\n",
            "Best dev perplexity = 283.7119180501348\n"
          ]
        }
      ],
      "source": [
        "bestPerplexity = float('inf')\n",
        "bestL1 = 0\n",
        "bestL2 = 0\n",
        "bestL3 = 0\n",
        "for i in range(0, 11):\n",
        "    for j in range(0, 11):\n",
        "        for k in range(1, 11):\n",
        "            if i + j + k == 10:\n",
        "                l1 = i/10\n",
        "                l2 = j/10\n",
        "                l3 = k/10\n",
        "                \n",
        "                perplexity = linearPerplexity(\"dev\", l1, l2, l3)\n",
        "                print(\"Linear interpolation dev perplexity (l1=\" + str(l1) + \", l2=\" + str(l2) + \", l3=\" + str(l3) + \") = \" + str(perplexity))\n",
        "                if perplexity < bestPerplexity:\n",
        "                    bestPerplexity = perplexity\n",
        "                    bestL1 = l1\n",
        "                    bestL2 = l2\n",
        "                    bestL3 = l3\n",
        "                    \n",
        "print(\"Best l1, l2, l3 = \" + str(bestL1) + \", \" + str(bestL2) + \", \" + str(bestL3))\n",
        "print(\"Best dev perplexity = \" + str(bestPerplexity))\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkf6IV0WVCQ"
      },
      "source": [
        "I tried all combinations of hyperparameters where λ1, λ2, λ3 are upto 1 decimal place.\n",
        "The best hyperparameter set I got is λ1 = 0.0, λ2 = 0.7, λ3 = 0.3, with a perplexity of 283.7119180501348.\n",
        "However, I do not rule out the possiblity that other sets of hyperparameters that go beyond 1 decimal place can achieve lower perplexity. If that is the case, there would be infintely more set that would need to be tested for further optimisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "##### 1.3.4 Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": [
        "Bayesian optimization could help find the optimal set of hyperparameters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vocab size 400000\n"
          ]
        }
      ],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 50\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
        "    return wv_from_bin\n",
        "\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Find most similar word\n",
        "Use cosine similarity to find the most similar word to each of these words. Report the most similar word and its cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer format: {word: [most similar word, cosine similarity]}\n",
            "{'dog': ['dogs', 0.81368625], 'whale': ['whales', 0.7918058], 'before': ['after', 0.8931248], 'however': ['although', 0.9336163], 'fabricate': ['fabricating', 0.61840194]}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "wordList = [\"dog\", \"whale\", \"before\", \"however\", \"fabricate\"]\n",
        "wordDict = {}\n",
        "vecKeys = wv_from_bin.index_to_key\n",
        "for word in wordList:\n",
        "  sim = -1\n",
        "  most = \"\"\n",
        "  for i in range(len(wv_from_bin)):\n",
        "    x = wv_from_bin[word]\n",
        "    cosine = np.dot(x,wv_from_bin[i])/(norm(x)*norm(wv_from_bin[i]))\n",
        "    curword = vecKeys[i]\n",
        "    if cosine >= sim and curword != word:\n",
        "      sim = cosine\n",
        "      most = curword\n",
        "  wordDict[word] = [most, sim]\n",
        "print(\"Answer format: {word: [most similar word, cosine similarity]}\")\n",
        "print(wordDict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Finding Analogies\n",
        "Use vector addition and subtraction to compute target vectors for the analogies below. After computing each target vector, find the top three candidates by cosine similarity. Report the candidates and their similarities to the target vector.\n",
        "\n",
        "- dog : puppy :: cat : ?:\n",
        "- speak : speaker :: sing : ?:\n",
        "- France : French :: England : ?:\n",
        "- France : wine :: England : ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dog : puppy :: cat : ?: ['cat', 'dog', 'dogs']\n",
            "corresponding similarities: [0.8246017, 0.745641, 0.6278181]\n",
            "speak : speaker :: sing : ?: ['sing', 'speak', 'singing']\n",
            "corresponding similarities: [0.7365669, 0.55140996, 0.548521]\n",
            "france : french :: england : ?: ['england', 'wales', 'ireland']\n",
            "corresponding similarities: [0.837254, 0.6498687, 0.6262423]\n",
            "france : wine :: england : ?: ['england', 'france', 'britain']\n",
            "corresponding similarities: [0.69291466, 0.6897908, 0.61666137]\n"
          ]
        }
      ],
      "source": [
        "# dog-puppy+cat\n",
        "wordList = [[\"dog\", \"puppy\", \"cat\"], [\"speak\", \"speaker\", \"sing\"], [\"france\", \"french\", \"england\"], [\"france\", \"wine\", \"england\"]]\n",
        "\n",
        "vecKeys = wv_from_bin.index_to_key\n",
        "for analogy in wordList:\n",
        "  targetVec = wv_from_bin[analogy[0]] - wv_from_bin[analogy[1]] + wv_from_bin[analogy[2]]\n",
        "  \n",
        "  candidateList = []\n",
        "  similarityList = []\n",
        "  for i in range(len(wv_from_bin)):\n",
        "    cosine = np.dot(targetVec,wv_from_bin[i])/(norm(targetVec)*norm(wv_from_bin[i]))\n",
        "    curword = vecKeys[i]\n",
        "    \n",
        "    if len(similarityList) < 3 or (len(similarityList) >= 3 and cosine >= similarityList[2]):\n",
        "      similarityList.append(cosine) \n",
        "      similarityList.sort(reverse=True)\n",
        "      candidateList.insert(similarityList.index(cosine), curword)\n",
        "  print(f\"{analogy[0]} : {analogy[1]} :: {analogy[2]} : ?:\", candidateList[0:3])\n",
        "  print(\"corresponding similarities:\", similarityList[0:3])\n",
        "# targetVec = wv_from_bin[\"dog\"] - wv_from_bin[\"puppy\"] + wv_from_bin[\"cat\"]\n",
        "# print(\"kitty\", np.dot(targetVec,wv_from_bin[\"kitty\"])/(norm(targetVec)*norm(wv_from_bin[\"kitty\"])))\n",
        "# print(\"kitten\", np.dot(targetVec,wv_from_bin[\"kitten\"])/(norm(targetVec)*norm(wv_from_bin[\"kitten\"])))\n",
        "# print(\"dogs\", np.dot(targetVec,wv_from_bin[\"dogs\"])/(norm(targetVec)*norm(wv_from_bin[\"dogs\"])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The syntax of the command is incorrect.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/classification\n",
        "!wget -O data/classification/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/train.txt\n",
        "!wget -O data/classification/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/dev.txt\n",
        "!wget -O data/classification/test-blind.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/test-blind.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unigram precision:  0.7893626002532714\n",
            "unigram recall:  0.7885829754988634\n",
            "unigram f1 score:  0.7887062789717657\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Read the training data\n",
        "train_data = []\n",
        "train_labels = []\n",
        "with open('data/classification/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.split()\n",
        "        sentiment = line[0]\n",
        "        train_data.append(' '.join(line[1:]))\n",
        "        train_labels.append(line[0])\n",
        "\n",
        "# Convert the text data into a matrix of token counts\n",
        "vectorizer = CountVectorizer()\n",
        "x = vectorizer.fit_transform(train_data)\n",
        "\n",
        "# Fit the LogisticRegression model\n",
        "logReg = LogisticRegression()\n",
        "logReg.fit(x, train_labels)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "test_data = []\n",
        "test_labels = []\n",
        "with open('data/classification/dev.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.split()\n",
        "        test_labels.append(line[0])\n",
        "        test_data.append(' '.join(line[1:]))\n",
        "test_data_transformed = vectorizer.transform(test_data)\n",
        "predictions = logReg.predict(test_data_transformed)\n",
        "\n",
        "print(\"unigram precision: \", precision_score(test_labels, predictions, average='macro'))\n",
        "print(\"unigram recall: \", recall_score(test_labels, predictions, average='macro'))\n",
        "print(\"unigram f1 score: \", f1_score(test_labels, predictions, average='macro'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bigram precision:  0.7213118279569892\n",
            "bigram recall:  0.7166161488591395\n",
            "bigram f1 score:  0.7159533898305085\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Read the training data\n",
        "train_data = []\n",
        "train_labels = []\n",
        "with open('data/classification/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.split()\n",
        "        train_data.append(' '.join(line[1:]))\n",
        "        train_labels.append(line[0])\n",
        "\n",
        "# Convert the text data into a matrix of token counts\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "x = vectorizer.fit_transform(train_data)\n",
        "\n",
        "# Fit the LogisticRegression model\n",
        "logReg = LogisticRegression()\n",
        "logReg.fit(x, train_labels)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "test_data = []\n",
        "test_labels = []\n",
        "with open('data/classification/dev.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.split()\n",
        "        test_labels.append(line[0])\n",
        "        test_data.append(' '.join(line[1:]))\n",
        "test_data_transformed = vectorizer.transform(test_data)\n",
        "predictions = logReg.predict(test_data_transformed)\n",
        "\n",
        "print(\"bigram precision: \", precision_score(test_labels, predictions, average='macro'))\n",
        "print(\"bigram recall: \", recall_score(test_labels, predictions, average='macro'))\n",
        "print(\"bigram f1 score: \", f1_score(test_labels, predictions, average='macro'))\n",
        "\n",
        "# print(x.shape)\n",
        "# print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GloVe Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'wv_from_bin' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\grace\\Desktop\\comp3361-asm\\A1\\A1.ipynb Cell 49\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X66sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m temp \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X66sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X66sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m wv_from_bin\u001b[39m.\u001b[39mkey_to_index:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X66sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m# avgVec.append(wv_from_bin[word])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X66sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39m# train_data.append(wv_from_bin[word])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X66sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         temp\u001b[39m.\u001b[39mappend(wv_from_bin[word])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A1/A1.ipynb#X66sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m#     train_data.append(wv_from_bin[\"<UNK>\"])\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'wv_from_bin' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Read the training data\n",
        "train_data = []\n",
        "train_labels = []\n",
        "with open('data/classification/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.split()\n",
        "        sentiment = line[0]\n",
        "        words = line[1:]\n",
        "        # print(len(line))\n",
        "        # avgVec = []\n",
        "        # avgVec = np.mean([wv_from_bin[word] for word in words if word in wv_from_bin.key_to_index], axis=0)\n",
        "        temp = []\n",
        "        for word in words:\n",
        "            if word in wv_from_bin.key_to_index:\n",
        "                # avgVec.append(wv_from_bin[word])\n",
        "                # train_data.append(wv_from_bin[word])\n",
        "                temp.append(wv_from_bin[word])\n",
        "            #     train_data.append(wv_from_bin[\"<UNK>\"])\n",
        "        train_data.append(temp)\n",
        "        # train_data = np.append(train_data, avgVec)\n",
        "        # train_data.append(np.mean(avgVec, axis=0))\n",
        "        # train_data.append(' '.join(line[1:]))\n",
        "        train_labels.append(line[0])\n",
        "\n",
        "# print(train_data)\n",
        "\n",
        "# print(len(train_data[0]))\n",
        "# print(np.array(train_data).shape)\n",
        "\n",
        "# Convert the text data into a matrix of token counts\n",
        "# vectorizer = CountVectorizer()\n",
        "# x = vectorizer.fit_transform(train_data)\n",
        "\n",
        "# Fit the LogisticRegression model\n",
        "logReg = LogisticRegression()\n",
        "logReg.fit(train_data, train_labels)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "test_data = []\n",
        "test_labels = []\n",
        "with open('data/classification/dev.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.split()\n",
        "        test_labels.append(line[0])\n",
        "        test_data.append(' '.join(line[1:]))\n",
        "# Convert the text data into a matrix of token counts\n",
        "vectorizer = CountVectorizer()\n",
        "test_data_transformed = vectorizer.transform(test_data)\n",
        "predictions = logReg.predict(test_data_transformed)\n",
        "\n",
        "print(\"bigram precision: \", precision_score(test_labels, predictions, average='macro'))\n",
        "print(\"bigram recall: \", recall_score(test_labels, predictions, average='macro'))\n",
        "print(\"bigram f1 score: \", f1_score(test_labels, predictions, average='macro'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the performance of three types of features on dev set. Report the weighted average precision, recall and F1-score for each feature set.\n",
        "\n",
        "| Feature | precision | recall | F1-score |\n",
        "| ----------- | --------- | ------ | -------- |\n",
        "| unigram     |0.7893626002532714|0.7885829754988634|0.7887062789717657|\n",
        "| bigram      |0.7213118279569892|0.7166161488591395|0.7159533898305085|\n",
        "| GloVe       |           |        |          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Better Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from gensim.parsing.preprocessing import remove_stopwords, stem_text, strip_punctuation, strip_multiple_whitespaces\n",
        "\n",
        "\n",
        "# Read the training data\n",
        "train_data = []\n",
        "train_labels = []\n",
        "with open('data/classification/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.split()\n",
        "        sentiment = line[0]\n",
        "        words = ' '.join(line[1:])\n",
        "        train_data.append((strip_multiple_whitespaces(strip_punctuation(words.replace('-RRB-', '').replace('-LRB-', '').lower()))))\n",
        "        train_labels.append(line[0])\n",
        "\n",
        "# print(train_data)\n",
        "# Convert the text data into a matrix of token counts\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 4))\n",
        "x = vectorizer.fit_transform(train_data)\n",
        "\n",
        "# Fit the LogisticRegression model\n",
        "logReg = LogisticRegression()\n",
        "logReg.fit(x, train_labels)\n",
        "\n",
        "# # Predict the labels for the test data\n",
        "# test_data = []\n",
        "# test_labels = []\n",
        "# with open('data/classification/dev.txt', 'r', encoding=\"utf-8\") as f:\n",
        "#     for line in f:\n",
        "#         line = line.split()\n",
        "#         test_labels.append(line[0])\n",
        "#         test_data.append(' '.join(line[1:]))\n",
        "# test_data_transformed = vectorizer.transform(test_data)\n",
        "# predictions = logReg.predict(test_data_transformed)\n",
        "\n",
        "# print(\"precision: \", precision_score(test_labels, predictions, average='macro'))\n",
        "# print(\"recall: \", recall_score(test_labels, predictions, average='macro'))\n",
        "# print(\"f1 score: \", f1_score(test_labels, predictions, average='macro'))\n",
        "\n",
        "test_data = []\n",
        "with open('data/classification/test-blind.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.split()\n",
        "        test_data.append(' '.join(line[1:]))\n",
        "test_data_transformed = vectorizer.transform(test_data)\n",
        "predictions = logReg.predict(test_data_transformed).tolist()\n",
        "\n",
        "with open('data/classification/3035927116.test.txt', 'w', encoding=\"utf-8\") as f:\n",
        "    with open('data/classification/test-blind.txt', 'r', encoding=\"utf-8\") as f2:\n",
        "        for line in f2:\n",
        "            f.write(predictions.pop(0) + \"\t\" + line)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.12.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
