{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.txt** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **1. clean all outputs and 2. run all cells in your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "# 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YsSAtTqt7Q8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The syntax of the command is incorrect.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/lm\n",
        "!wget -O data/lm/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/train.txt\n",
        "!wget -O data/lm/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/dev.txt\n",
        "!wget -O data/lm/test.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "## 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q-rNT_QL8Dvt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary = 26601\n"
          ]
        }
      ],
      "source": [
        "with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    train = dict()\n",
        "    for line in f:\n",
        "        line = '<START> ' + line + ' <END>'     # to add <START> and <END> to the start and end of each sentence \n",
        "        tmp = line.split()\n",
        "        for word in tmp:\n",
        "            if word in train.keys():            # if in dict, increment counter\n",
        "                train[word] += 1\n",
        "            else:\n",
        "                train[word] = 1                 # if not in dict, add new entry\n",
        "\n",
        "    countUNK = 0\n",
        "    for key in list(train.keys()):              # find all words with freq < 3, sum their counts\n",
        "        if train[key] < 3:\n",
        "            countUNK += train[key]\n",
        "            del train[key]\n",
        "    train['<UNK>'] = countUNK\n",
        "    print(\"Size of vocabulary = \" + str(len(train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "Size of vocabulary = 26601\n",
        "\n",
        "Number of parameters of n-gram models = (size of vocabulary) ^ n = (26601) ^ n\n",
        "eg unigram: 26601 ^ 1\n",
        "eg bigram: 26601 ^ 2\n",
        "eg trigram: 26601 ^ 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "## 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ACSfNZGE8Yw2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "72045\n",
            "0.04293396708783213\n",
            "1678042\n",
            "26601\n",
            "9\n",
            "5.3633937648759684e-06\n",
            "Unigram probability of 'the' = 0.04293396708783213\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "unigramDict = dict()\n",
        "bigramDict = dict()\n",
        "\n",
        "# build unigram\n",
        "def unigram():\n",
        "    for key in train.keys():\n",
        "        unigramDict[key] = float(train[key]) / sum(train.values())\n",
        "\n",
        "def unigramProb(word):\n",
        "    if word in unigramDict.keys():\n",
        "        return unigramDict[word]\n",
        "    else:\n",
        "        return unigramDict['<UNK>']\n",
        "\n",
        "# build bigram\n",
        "def bigram():\n",
        "    with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            for i in range(len(tmp) - 1):\n",
        "                if tmp[i] not in train.keys():\n",
        "                    tmp[i] = '<UNK>'\n",
        "                if tmp[i+1] not in train.keys():\n",
        "                    tmp[i+1] = '<UNK>'\n",
        "                    \n",
        "                if (tmp[i], tmp[i+1]) in bigramDict.keys():\n",
        "                    bigramDict[(tmp[i], tmp[i+1])] += 1\n",
        "                else:\n",
        "                    bigramDict[(tmp[i], tmp[i+1])] = 1\n",
        "    for key in bigramDict.keys():\n",
        "        # bigramDict[key] = float(bigramDict[key])     # P(w2|w1) = C(w1, w2) / C(w1)\n",
        "        bigramDict[key] = float(bigramDict[key]) / train[key[0]]    # P(w2|w1) = C(w1, w2) / C(w1)\n",
        "        # bigramDict[key] = train[key[0]]    # P(w2|w1) = C(w1, w2) / C(w1)\n",
        "                    \n",
        "unigram()\n",
        "# bigram()\n",
        "print(train['the'])\n",
        "print(unigramDict['the'])\n",
        "print(sum(train.values()))\n",
        "print(str(len(train)))\n",
        "print(train['exceeding'])\n",
        "print(unigramDict['exceeding'])\n",
        "print(\"Unigram probability of 'the' = \" + str(unigramDict['the']))\n",
        "# print(\"Bigram probability of 'the' given 'the' = \" + str(bigramDict[('the', 'the')]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram train perplexity = 866.6059171242847\n",
            "Unigram dev perplexity = 795.3775132279369\n",
            "Bigram train perplexity = inf\n",
            "Bigram dev perplexity = inf\n"
          ]
        }
      ],
      "source": [
        "def unigramTrainPerplexity():\n",
        "    with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        N = 0\n",
        "        sum = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            # print(tmp)\n",
        "            prob = 1\n",
        "            for word in tmp:\n",
        "                N += 1\n",
        "                # prob *= unigramProb(word)\n",
        "                prob += math.log2(unigramProb(word))\n",
        "                # print(prob, unigramProb(word), word, line)\n",
        "            sum += (prob)\n",
        "    return 2 ** (-sum / N)\n",
        "    \n",
        "def unigramDevPerplexity():\n",
        "    with open('data/lm/dev.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        N = 0\n",
        "        sum = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            # print(tmp)\n",
        "            prob = 1\n",
        "            for word in tmp:\n",
        "                N += 1\n",
        "                # prob *= unigramProb(word)\n",
        "                prob += math.log2(unigramProb(word))\n",
        "                # print(prob, unigramProb(word), word, line)\n",
        "            sum += (prob)\n",
        "    return 2 ** (-sum / N)\n",
        "\n",
        "def bigramTrainPerplexity():\n",
        "    with open('data/lm/train.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        perplexity = 1\n",
        "        N = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            for i in range(len(tmp) - 1):\n",
        "                N += 1\n",
        "                if (tmp[i], tmp[i+1]) in bigramDict.keys():\n",
        "                    perplexity *= 1 / bigramDict[(tmp[i], tmp[i+1])]\n",
        "                else:\n",
        "                    perplexity *= 1 / unigramProb(tmp[i+1])\n",
        "        perplexity = math.pow(perplexity, 1/float(N))\n",
        "        return perplexity\n",
        "    \n",
        "def bigramDevPerplexity():\n",
        "    with open('data/lm/dev.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        perplexity = 1\n",
        "        N = 0\n",
        "        for line in f:\n",
        "            line = '<START> ' + line + ' <END>'\n",
        "            tmp = line.split()\n",
        "            for i in range(len(tmp) - 1):\n",
        "                N += 1\n",
        "                if (tmp[i], tmp[i+1]) in bigramDict.keys():\n",
        "                    perplexity *= 1 / bigramDict[(tmp[i], tmp[i+1])]\n",
        "                else:\n",
        "                    perplexity *= 1 / unigramProb(tmp[i+1])\n",
        "        perplexity = math.pow(perplexity, 1/float(N))\n",
        "        return perplexity\n",
        "    \n",
        "print(\"Unigram train perplexity = \" + str(unigramTrainPerplexity()))\n",
        "print(\"Unigram dev perplexity = \" + str(unigramDevPerplexity()))\n",
        "print(\"Bigram train perplexity = \" + str(bigramTrainPerplexity()))\n",
        "print(\"Bigram dev perplexity = \" + str(bigramDevPerplexity()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4gcgL--Ylh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "## 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "### 1.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93_yLu9dVr0C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "#### 1.3.2 Add-k smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhcuJWo7VuNg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOUpNXYVuNh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "### 1.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4N_XuN6WVCQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkf6IV0WVCQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "##### 1.3.4 Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vocab size 400000\n"
          ]
        }
      ],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 50\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
        "    return wv_from_bin\n",
        "\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Find most similar word\n",
        "Use cosine similarity to find the most similar word to each of these words. Report the most similar word and its cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer format: {word: [most similar word, cosine similarity]}\n",
            "{'dog': ['dogs', 0.81368625], 'whale': ['whales', 0.7918058], 'before': ['after', 0.8931248], 'however': ['although', 0.9336163], 'fabricate': ['fabricating', 0.61840194]}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "wordList = [\"dog\", \"whale\", \"before\", \"however\", \"fabricate\"]\n",
        "wordDict = {}\n",
        "vecKeys = wv_from_bin.index_to_key\n",
        "for word in wordList:\n",
        "  sim = -1\n",
        "  most = \"\"\n",
        "  for i in range(len(wv_from_bin)):\n",
        "    x = wv_from_bin[word]\n",
        "    cosine = np.dot(x,wv_from_bin[i])/(norm(x)*norm(wv_from_bin[i]))\n",
        "    curword = vecKeys[i]\n",
        "    if cosine >= sim and curword != word:\n",
        "      sim = cosine\n",
        "      most = curword\n",
        "  wordDict[word] = [most, sim]\n",
        "print(\"Answer format: {word: [most similar word, cosine similarity]}\")\n",
        "print(wordDict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Finding Analogies\n",
        "Use vector addition and subtraction to compute target vectors for the analogies below. After computing each target vector, find the top three candidates by cosine similarity. Report the candidates and their similarities to the target vector.\n",
        "\n",
        "- dog : puppy :: cat : ?:\n",
        "- speak : speaker :: sing : ?:\n",
        "- France : French :: England : ?:\n",
        "- France : wine :: England : ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dog : puppy :: cat : ?: ['cat', 'dog', 'dogs']\n",
            "corresponding similarities: [0.8246017, 0.745641, 0.6278181]\n",
            "speak : speaker :: sing : ?: ['sing', 'speak', 'singing']\n",
            "corresponding similarities: [0.7365669, 0.55140996, 0.548521]\n",
            "france : french :: england : ?: ['england', 'wales', 'ireland']\n",
            "corresponding similarities: [0.837254, 0.6498687, 0.6262423]\n",
            "france : wine :: england : ?: ['england', 'france', 'britain']\n",
            "corresponding similarities: [0.69291466, 0.6897908, 0.61666137]\n"
          ]
        }
      ],
      "source": [
        "# dog-puppy+cat\n",
        "wordList = [[\"dog\", \"puppy\", \"cat\"], [\"speak\", \"speaker\", \"sing\"], [\"france\", \"french\", \"england\"], [\"france\", \"wine\", \"england\"]]\n",
        "\n",
        "vecKeys = wv_from_bin.index_to_key\n",
        "for analogy in wordList:\n",
        "  targetVec = wv_from_bin[analogy[0]] - wv_from_bin[analogy[1]] + wv_from_bin[analogy[2]]\n",
        "  \n",
        "  candidateList = []\n",
        "  similarityList = []\n",
        "  for i in range(len(wv_from_bin)):\n",
        "    cosine = np.dot(targetVec,wv_from_bin[i])/(norm(targetVec)*norm(wv_from_bin[i]))\n",
        "    curword = vecKeys[i]\n",
        "    \n",
        "    if len(similarityList) < 3 or (len(similarityList) >= 3 and cosine >= similarityList[2]):\n",
        "      similarityList.append(cosine) \n",
        "      similarityList.sort(reverse=True)\n",
        "      candidateList.insert(similarityList.index(cosine), curword)\n",
        "  print(f\"{analogy[0]} : {analogy[1]} :: {analogy[2]} : ?:\", candidateList[0:3])\n",
        "  print(\"corresponding similarities:\", similarityList[0:3])\n",
        "# targetVec = wv_from_bin[\"dog\"] - wv_from_bin[\"puppy\"] + wv_from_bin[\"cat\"]\n",
        "# print(\"kitty\", np.dot(targetVec,wv_from_bin[\"kitty\"])/(norm(targetVec)*norm(wv_from_bin[\"kitty\"])))\n",
        "# print(\"kitten\", np.dot(targetVec,wv_from_bin[\"kitten\"])/(norm(targetVec)*norm(wv_from_bin[\"kitten\"])))\n",
        "# print(\"dogs\", np.dot(targetVec,wv_from_bin[\"dogs\"])/(norm(targetVec)*norm(wv_from_bin[\"dogs\"])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The syntax of the command is incorrect.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/classification\n",
        "!wget -O data/classification/train.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/train.txt\n",
        "!wget -O data/classification/dev.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/dev.txt\n",
        "!wget -O data/classification/test-blind.txt https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A1/data/classification/test-blind.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bigram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GloVe Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the performance of three types of features on dev set. Report the weighted average precision, recall and F1-score for each feature set.\n",
        "\n",
        "| Feature | precision | recall | F1-score |\n",
        "| ----------- | --------- | ------ | -------- |\n",
        "| unigram     |           |        |          |\n",
        "| bigram      |           |        |          |\n",
        "| GloVe       |           |        |          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Better Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.12.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
