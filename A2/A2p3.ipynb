{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KinZKVrXtkoC"
      },
      "source": [
        "# COMP3361 Part 1: Building a Transformer Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjP56aVdtkoD"
      },
      "source": [
        "Note: You should finish your code solution of Part 1 & 2 with A2p12.tgz. For Q2 & Q3, you should include your writeup in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ7M_e4ItkoE"
      },
      "source": [
        "## Q2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLqeIvaztkoE"
      },
      "source": [
        "## Q3:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byvJH-2HtkoE"
      },
      "source": [
        "# COMP3361 Part 3: Generation with Large Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWR51XJotkoF"
      },
      "source": [
        "## Load model and tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se2g3gTqtkoF"
      },
      "source": [
        "In this section, we will use [CodeLlama-7B](https://huggingface.co/codellama/CodeLlama-7b-hf) as the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EU4clXj7uta9",
        "outputId": "9fd5b83d-be4d-43b9-e3ea-5643af479b29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D1cF8N6xt5J7"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Dict, Any\n",
        "import os\n",
        "import json\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import locale\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "# os.envion['LC_ALL'] = 'en_US.UTF-8'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9Q6zTg8BuFCX"
      },
      "outputs": [],
      "source": [
        "# You will load this model in a 4-bit inference mode to optimize resource usage.\n",
        "# This setup involves implementing a class named LLM that facilitates loading the model\n",
        "# and generating text completions based on provided prompts\n",
        "class LLM(object):\n",
        "    def __init__(self, model_name=\"codellama/CodeLlama-7b-hf\"):\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
        "        # pass\n",
        "\n",
        "    def generate(self, prompts: List[str], **kwargs) -> List[str]:\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "        modelInput = self.tokenizer(\n",
        "            prompts, return_tensors=\"pt\", padding=True\n",
        "        ).to(\"cuda\")\n",
        "        # self.tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
        "\n",
        "        return self.model.generate(**modelInput)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "59OjAXaewjpt",
        "outputId": "c8eaa07a-c0fd-4330-bc4e-e68a4e2d4250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343,
          "referenced_widgets": [
            "be4e87d7de384aefa4d56c581dd76336",
            "9fd22a85334049dfaf494b7978649e0d",
            "583d01deed294644bf5ccaaf5ba01339",
            "dfa3d2c54ef34cd6b349b4efd468d3b7",
            "e1c68ff2266040c4b505760380ba845f",
            "329564be45b84eb3abd361eb3d6b2ffd",
            "a1f424e5f58c46269a3dfb4521bf4bdd",
            "2348915f08644293ad6a4c847731eb1a",
            "bc37a4cbf44a40688fad4c9599d4d8b7",
            "6383b06cdafb444d95c14adc9ec002b1",
            "77bcbb24ed3749968169d7e5a4ac8348"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be4e87d7de384aefa4d56c581dd76336"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A list of colors: red, blue, green, yellow, orange, purple, brown',\n",
              " 'Portugal is a small town in the state of New York, United']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "llm = LLM()\n",
        "\n",
        "generated_ids = llm.generate([\"A list of colors: red, blue\", \"Portugal is\"])\n",
        "llm.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "# type((List([\"A list of colors: red, blue\", \"Portugal is\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lC_-chU1uHqL"
      },
      "outputs": [],
      "source": [
        "class Evaluator(ABC):\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    @abstractmethod\n",
        "    def load_data(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def build_prompts(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def postprocess_output(self, output: str) -> str:\n",
        "        pass\n",
        "\n",
        "    def generate_completions(self, prompts: List[str], batch_size=4, **kwargs) -> List[str]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def evaluate(self, evalset_file, batch_size=4, save_dir=\"outputs\", max_new_tokens=128, **kwargs):\n",
        "        dataset = self.load_data(evalset_file)\n",
        "        prompts = self.build_prompts(dataset)\n",
        "        outputs = self.generate_completions(prompts, batch_size=batch_size, max_new_tokens=max_new_tokens, **kwargs)\n",
        "\n",
        "        predictions = []\n",
        "        for i, (example, prompt, output) in enumerate(zip(dataset, prompts, outputs)):\n",
        "            prediction = {\n",
        "                \"task_id\": example.get(\"task_id\", f\"task_{i}\"),\n",
        "                \"prompt\": prompt,\n",
        "                \"completion\": self.postprocess_output(output)\n",
        "            }\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        # Save predictions to file\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        prediction_save_path = os.path.join(save_dir, f\"{type(self).__name__}_predictions.jsonl\")\n",
        "        with open(prediction_save_path, \"w\") as fout:\n",
        "            for pred in predictions:\n",
        "                fout.write(json.dumps(pred) + \"\\n\")\n",
        "\n",
        "        # Calculate metrics and print results\n",
        "        results = self.calculate_metrics(predictions, dataset)\n",
        "        print(f\"Results for {type(self).__name__}: {results}\")\n",
        "\n",
        "    @abstractmethod\n",
        "    def calculate_metrics(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # FOR TESTING ONLY\n",
        "# # Delete before submission\n",
        "\n",
        "# # Free GPU\n",
        "# from numba import cuda\n",
        "# device = cuda.get_current_device()\n",
        "# device.reset()"
      ],
      "metadata": {
        "id": "4gRA6Rfe7iL5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8obHGZsMtkoI"
      },
      "source": [
        "## Zero-shot Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iEw3OvExutT9",
        "outputId": "bca77110-9a29-47ae-dc04-874cd28875c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-04 10:01:50--  https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/human_eval/__init__.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 0 [text/plain]\n",
            "Saving to: ‘human_eval/__init__.py’\n",
            "\n",
            "human_eval/__init__     [ <=>                ]       0  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-04 10:01:50 (0.00 B/s) - ‘human_eval/__init__.py’ saved [0/0]\n",
            "\n",
            "--2024-03-04 10:01:50--  http://human_eval/\n",
            "Resolving human_eval (human_eval)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘human_eval’\n",
            "--2024-03-04 10:01:50--  https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/human_eval/data.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1528 (1.5K) [text/plain]\n",
            "Saving to: ‘human_eval/data.py’\n",
            "\n",
            "human_eval/data.py  100%[===================>]   1.49K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-04 10:01:50 (25.6 MB/s) - ‘human_eval/data.py’ saved [1528/1528]\n",
            "\n",
            "FINISHED --2024-03-04 10:01:50--\n",
            "Total wall clock time: 0.2s\n",
            "Downloaded: 1 files, 1.5K in 0s (25.6 MB/s)\n",
            "--2024-03-04 10:01:50--  https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/human_eval/evaluation.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3463 (3.4K) [text/plain]\n",
            "Saving to: ‘human_eval/evaluation.py’\n",
            "\n",
            "human_eval/evaluati 100%[===================>]   3.38K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-04 10:01:50 (54.3 MB/s) - ‘human_eval/evaluation.py’ saved [3463/3463]\n",
            "\n",
            "--2024-03-04 10:01:50--  http://human_eval/\n",
            "Resolving human_eval (human_eval)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘human_eval’\n",
            "--2024-03-04 10:01:50--  https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/human_eval/execution.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6404 (6.3K) [text/plain]\n",
            "Saving to: ‘human_eval/execution.py’\n",
            "\n",
            "human_eval/executio 100%[===================>]   6.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-04 10:01:50 (73.0 MB/s) - ‘human_eval/execution.py’ saved [6404/6404]\n",
            "\n",
            "FINISHED --2024-03-04 10:01:50--\n",
            "Total wall clock time: 0.1s\n",
            "Downloaded: 1 files, 6.3K in 0s (73.0 MB/s)\n",
            "--2024-03-04 10:01:50--  https://github.com/openai/human-eval/raw/master/data/HumanEval.jsonl.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/openai/human-eval/master/data/HumanEval.jsonl.gz [following]\n",
            "--2024-03-04 10:01:51--  https://raw.githubusercontent.com/openai/human-eval/master/data/HumanEval.jsonl.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44877 (44K) [application/octet-stream]\n",
            "Saving to: ‘data/humaneval/HumanEval.jsonl.gz’\n",
            "\n",
            "data/humaneval/Huma 100%[===================>]  43.83K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-03-04 10:01:51 (4.19 MB/s) - ‘data/humaneval/HumanEval.jsonl.gz’ saved [44877/44877]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p human_eval\n",
        "!wget -O human_eval/__init__.py https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/human_eval/__init__.py\n",
        "!wget -O human_eval/data.py human_eval https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/human_eval/data.py\n",
        "!wget -O human_eval/evaluation.py https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/human_eval/evaluation.py\n",
        "!wget -O human_eval/execution.py human_eval https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/human_eval/execution.py\n",
        "\n",
        "!mkdir -p data/humaneval\n",
        "!wget -O data/humaneval/HumanEval.jsonl.gz https://github.com/openai/human-eval/raw/master/data/HumanEval.jsonl.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4xtsnKpjuKCI"
      },
      "outputs": [],
      "source": [
        "from human_eval.data import read_problems\n",
        "from human_eval.evaluation import evaluate_functional_correctness\n",
        "\n",
        "class HumanEvalEvaluator(Evaluator):\n",
        "    def load_data(self, evalset_file=\"data/humaneval/HumanEval.jsonl.gz\") -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Load the humaneval dataset\n",
        "        :param evalset_file: path to the humaneval dataset file\n",
        "        :return: list of examples\n",
        "        \"\"\"\n",
        "        return list(read_problems(evalset_file).values())\n",
        "\n",
        "    def build_prompts(self, dataset) -> List[str]:\n",
        "        \"\"\"\n",
        "        Build zero-shot prompts from the humaneval dataset.\n",
        "        \"\"\"\n",
        "        prompts = [example[\"prompt\"] for example in dataset]\n",
        "        return prompts\n",
        "\n",
        "    def postprocess_output(self, output: str) -> str:\n",
        "        stop_sequences=[\"\\nclass\", \"\\ndef\", \"\\n#\", \"\\nif\", \"\\nprint\"]\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def calculate_metrics(self, predictions, dataset):\n",
        "        pass_at_k_results = evaluate_functional_correctness(\n",
        "            sample_file=os.path.join(\"outputs\", f\"{type(self).__name__}_predictions.jsonl\"),\n",
        "            k=[1],\n",
        "            problems={example[\"task_id\"]: example for example in dataset},\n",
        "            n_workers=64\n",
        "        )\n",
        "        return pass_at_k_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J91pEUb_uXgB",
        "outputId": "6ebdb3d1-51c4-4dcf-fc51-5077539d92d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Evaluator.evaluate() missing 1 required positional argument: 'evalset_file'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3121b5ae6d97>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhuman_eval_evaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHumanEvalEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhuman_eval_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Evaluator.evaluate() missing 1 required positional argument: 'evalset_file'"
          ]
        }
      ],
      "source": [
        "human_eval_evaluator = HumanEvalEvaluator(llm)\n",
        "human_eval_evaluator.evaluate(batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpDMsRMHtkoJ"
      },
      "source": [
        "## Few-shot Math Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AGOtyjcuPHJ"
      },
      "outputs": [],
      "source": [
        "GSM_EXAMPLARS = [\n",
        "    {\n",
        "        \"question\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n",
        "        \"cot_answer\": \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\\"\\\"\\\"\\n    trees_initial = 15\\n    trees_after = 21\\n    trees_added = trees_after - trees_initial\\n    result = trees_added\\n    return result\",\n",
        "        \"short_answer\": \"6\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n",
        "        \"cot_answer\": \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\\"\\\"\\\"\\n    cars_initial = 3\\n    cars_arrived = 2\\n    total_cars = cars_initial + cars_arrived\\n    result = total_cars\\n    return result\",\n",
        "        \"short_answer\": \"5\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n",
        "        \"cot_answer\": \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\\"\\\"\\\"\\n    leah_chocolates = 32\\n    sister_chocolates = 42\\n    total_chocolates = leah_chocolates + sister_chocolates\\n    chocolates_eaten = 35\\n    chocolates_left = total_chocolates - chocolates_eaten\\n    result = chocolates_left\\n    return result\",\n",
        "        \"short_answer\": \"39\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",\n",
        "        \"cot_answer\": \"Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\\"\\\"\\\"\\n    jason_lollipops_initial = 20\\n    jason_lollipops_after = 12\\n    denny_lollipops = jason_lollipops_initial - jason_lollipops_after\\n    result = denny_lollipops\\n    return result\",\n",
        "        \"short_answer\": \"8\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",\n",
        "        \"cot_answer\": \"Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\\"\\\"\\\"\\n    toys_initial = 5\\n    mom_toys = 2\\n    dad_toys = 2\\n    total_received = mom_toys + dad_toys\\n    total_toys = toys_initial + total_received\\n    result = total_toys\\n    return result\",\n",
        "        \"short_answer\": \"9\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",\n",
        "        \"cot_answer\": \"There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\\"\\\"\\\"\\n    toys_initial = 5\\n    mom_toys = 2\\n    dad_toys = 2\\n    total_received = mom_toys + dad_toys\\n    total_toys = toys_initial + total_received\\n    result = total_toys\\n    return result\",\n",
        "        \"short_answer\": \"29\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",\n",
        "        \"cot_answer\": \"Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\\"\\\"\\\"\\n    golf_balls_initial = 58\\n    golf_balls_lost_tuesday = 23\\n    golf_balls_lost_wednesday = 2\\n    golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday\\n    result = golf_balls_left\\n    return result\",\n",
        "        \"short_answer\": \"33\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\",\n",
        "        \"cot_answer\": \"Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\\"\\\"\\\"\\n    money_initial = 23\\n    bagels = 5\\n    bagel_cost = 3\\n    money_spent = bagels * bagel_cost\\n    money_left = money_initial - money_spent\\n    result = money_left\\n    return result\",\n",
        "        \"short_answer\": \"8\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbAdW1gMuR0t"
      },
      "outputs": [],
      "source": [
        "class GSM8KEvaluator(Evaluator):\n",
        "    def load_data(self, evalset_file=\"gsm8k\") -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Load the GSM8K dataset https://huggingface.co/datasets/gsm8k with Huggingface datasets library\n",
        "        Load the first 100 examples from the test split in main subset.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build_prompts(self, dataset, n_shot=8, demos=GSM_EXAMPLARS):\n",
        "        \"\"\"\n",
        "        Build few-shot prompts from the GSM8K dataset. Use\n",
        "        :param dataset: list of examples\n",
        "        :param n_shot: number of examples to use for few-shot learning\n",
        "        :param demos: list of demonstrator examples\n",
        "        :return: list of prompts\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def postprocess_output(self, output: str) -> str:\n",
        "        \"\"\"\n",
        "        Postprocess the output from the language model.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def calculate_metrics(self, predictions, dataset):\n",
        "        \"\"\"\n",
        "        Calculate metrics for the GSM8K dataset\n",
        "        \"\"\"\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AngLuranuVVo"
      },
      "outputs": [],
      "source": [
        "gsm8k_evaluator = GSM8KEvaluator(llm)\n",
        "gsm8k_evaluator.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO8otOhhtkoK"
      },
      "source": [
        "## Few-shot Chain-of Thought Math Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkVZ_9oGtkoK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GSM8KCoTEvaluator(GSM8KEvaluator):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYw2dxIJtkoK"
      },
      "outputs": [],
      "source": [
        "gsm8k_cot_evaluator = GSM8KCoTEvaluator(llm)\n",
        "gsm8k_cot_evaluator.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJci4R4mtkoK"
      },
      "source": [
        "## Few-shot Program-of Thought Math Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J94KmfOxtkoK"
      },
      "outputs": [],
      "source": [
        "!pip install timeout-decorator Pebble\n",
        "!wget -O python_executor.py https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/python_executor.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7qWrFuqtkoL"
      },
      "outputs": [],
      "source": [
        "from python_executor import PythonExecutor\n",
        "executor = PythonExecutor(get_answer_expr='solution()')\n",
        "\n",
        "codes = [\n",
        "    \"def solution():\\n    return 1 + 1\",\n",
        "    \"def solution():\\n    return 2 * 2\",\n",
        "]\n",
        "\n",
        "predictions = []\n",
        "runtime_errors = []\n",
        "for pred, err in executor.batch_apply(codes):\n",
        "    predictions.append(str(pred))\n",
        "    runtime_errors.append(str(err['exec_info']).strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgRjswkYtkoL"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqZvQnJitkoL"
      },
      "outputs": [],
      "source": [
        "class GSM8KPoTEvaluator(Evaluator):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVw4eXWstkoL"
      },
      "source": [
        "|                    | GSM8K |\n",
        "|--------------------|-------|\n",
        "| Direct Prompting   |       |\n",
        "| Chain-of-Thought   |       |\n",
        "| Program-of-Thought |       |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "be4e87d7de384aefa4d56c581dd76336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fd22a85334049dfaf494b7978649e0d",
              "IPY_MODEL_583d01deed294644bf5ccaaf5ba01339",
              "IPY_MODEL_dfa3d2c54ef34cd6b349b4efd468d3b7"
            ],
            "layout": "IPY_MODEL_e1c68ff2266040c4b505760380ba845f"
          }
        },
        "9fd22a85334049dfaf494b7978649e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_329564be45b84eb3abd361eb3d6b2ffd",
            "placeholder": "​",
            "style": "IPY_MODEL_a1f424e5f58c46269a3dfb4521bf4bdd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "583d01deed294644bf5ccaaf5ba01339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2348915f08644293ad6a4c847731eb1a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc37a4cbf44a40688fad4c9599d4d8b7",
            "value": 2
          }
        },
        "dfa3d2c54ef34cd6b349b4efd468d3b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6383b06cdafb444d95c14adc9ec002b1",
            "placeholder": "​",
            "style": "IPY_MODEL_77bcbb24ed3749968169d7e5a4ac8348",
            "value": " 2/2 [01:06&lt;00:00, 30.60s/it]"
          }
        },
        "e1c68ff2266040c4b505760380ba845f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "329564be45b84eb3abd361eb3d6b2ffd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1f424e5f58c46269a3dfb4521bf4bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2348915f08644293ad6a4c847731eb1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc37a4cbf44a40688fad4c9599d4d8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6383b06cdafb444d95c14adc9ec002b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77bcbb24ed3749968169d7e5a4ac8348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}