{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graceskw/comp3361-asm/blob/main/A2/A2p3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KinZKVrXtkoC"
      },
      "source": [
        "# COMP3361 Part 1: Building a Transformer Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjP56aVdtkoD"
      },
      "source": [
        "Note: You should finish your code solution of Part 1 & 2 with A2p12.tgz. For Q2 & Q3, you should include your writeup in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ7M_e4ItkoE"
      },
      "source": [
        "## Q2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLqeIvaztkoE"
      },
      "source": [
        "## Q3:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byvJH-2HtkoE"
      },
      "source": [
        "# COMP3361 Part 3: Generation with Large Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWR51XJotkoF"
      },
      "source": [
        "## Load model and tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se2g3gTqtkoF"
      },
      "source": [
        "In this section, we will use [CodeLlama-7B](https://huggingface.co/codellama/CodeLlama-7b-hf) as the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EU4clXj7uta9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "     ---------------------------------------- 0.0/130.7 kB ? eta -:--:--\n",
            "     -------- ---------------------------- 30.7/130.7 kB 660.6 kB/s eta 0:00:01\n",
            "     -------------------------------------- 130.7/130.7 kB 1.5 MB/s eta 0:00:00\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
            "  Downloading huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\grace\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (23.2)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\python312\\lib\\site-packages (from transformers) (2023.12.25)\n",
            "Collecting requests (from transformers)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers)\n",
            "  Downloading safetensors-0.4.2-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\python312\\lib\\site-packages (from transformers) (4.66.2)\n",
            "Collecting pyarrow>=12.0.0 (from datasets)\n",
            "  Downloading pyarrow-15.0.0-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets)\n",
            "  Downloading pandas-2.2.1-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets)\n",
            "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.9.3-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: psutil in c:\\users\\grace\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (5.9.8)\n",
            "Collecting torch>=1.10.0 (from accelerate)\n",
            "  Using cached torch-2.2.1-cp312-cp312-win_amd64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: scipy in c:\\python312\\lib\\site-packages (from bitsandbytes) (1.12.0)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.5-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.4-cp312-cp312-win_amd64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
            "  Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers)\n",
            "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
            "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
            "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: sympy in c:\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Collecting networkx (from torch>=1.10.0->accelerate)\n",
            "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting jinja2 (from torch>=1.10.0->accelerate)\n",
            "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\grace\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\grace\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets)\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\grace\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.10.0->accelerate)\n",
            "  Using cached MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\python312\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.6/8.5 MB 12.9 MB/s eta 0:00:01\n",
            "   ------ --------------------------------- 1.4/8.5 MB 14.5 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 2.2/8.5 MB 17.8 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 3.2/8.5 MB 16.8 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 4.1/8.5 MB 17.3 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 5.0/8.5 MB 17.8 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 5.9/8.5 MB 18.8 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 6.8/8.5 MB 18.8 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 7.7/8.5 MB 18.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------  8.5/8.5 MB 18.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.5/8.5 MB 18.2 MB/s eta 0:00:00\n",
            "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "   ---------------------------------------- 0.0/510.5 kB ? eta -:--:--\n",
            "   --------------------------------------- 510.5/510.5 kB 16.1 MB/s eta 0:00:00\n",
            "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "   ---------------------------------------- 0.0/84.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 84.1/84.1 kB ? eta 0:00:00\n",
            "Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "   ---------------------------------------- 0.0/280.0 kB ? eta -:--:--\n",
            "   --------------------------------------- 280.0/280.0 kB 18.0 MB/s eta 0:00:00\n",
            "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "   ---------------------------------------- 0.0/105.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.0/105.0 MB 20.0 MB/s eta 0:00:06\n",
            "    --------------------------------------- 1.8/105.0 MB 18.7 MB/s eta 0:00:06\n",
            "    --------------------------------------- 2.6/105.0 MB 18.3 MB/s eta 0:00:06\n",
            "   - -------------------------------------- 3.5/105.0 MB 18.7 MB/s eta 0:00:06\n",
            "   - -------------------------------------- 4.5/105.0 MB 19.0 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 5.4/105.0 MB 19.2 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 5.7/105.0 MB 19.1 MB/s eta 0:00:06\n",
            "   -- ------------------------------------- 5.9/105.0 MB 15.8 MB/s eta 0:00:07\n",
            "   -- ------------------------------------- 6.8/105.0 MB 16.1 MB/s eta 0:00:07\n",
            "   -- ------------------------------------- 7.6/105.0 MB 16.3 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 8.5/105.0 MB 16.5 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 9.4/105.0 MB 16.6 MB/s eta 0:00:06\n",
            "   --- ------------------------------------ 10.3/105.0 MB 16.8 MB/s eta 0:00:06\n",
            "   ---- ----------------------------------- 11.0/105.0 MB 16.8 MB/s eta 0:00:06\n",
            "   ---- ----------------------------------- 11.7/105.0 MB 16.4 MB/s eta 0:00:06\n",
            "   ---- ----------------------------------- 12.4/105.0 MB 16.4 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 13.1/105.0 MB 16.4 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 13.9/105.0 MB 16.0 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 14.7/105.0 MB 15.6 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 15.7/105.0 MB 16.0 MB/s eta 0:00:06\n",
            "   ------ --------------------------------- 16.5/105.0 MB 17.2 MB/s eta 0:00:06\n",
            "   ------ --------------------------------- 17.5/105.0 MB 17.7 MB/s eta 0:00:05\n",
            "   ------ --------------------------------- 18.3/105.0 MB 17.7 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 19.1/105.0 MB 17.7 MB/s eta 0:00:05\n",
            "   ------- -------------------------------- 20.1/105.0 MB 17.7 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 21.0/105.0 MB 17.7 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 21.8/105.0 MB 18.2 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 22.8/105.0 MB 18.7 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 23.6/105.0 MB 18.7 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 24.5/105.0 MB 18.7 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 25.4/105.0 MB 19.2 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 26.2/105.0 MB 19.3 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 27.1/105.0 MB 18.7 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 28.0/105.0 MB 18.7 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 28.9/105.0 MB 18.7 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 29.8/105.0 MB 19.3 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 30.6/105.0 MB 18.7 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 31.4/105.0 MB 18.7 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 32.0/105.0 MB 18.2 MB/s eta 0:00:05\n",
            "   ------------ --------------------------- 32.7/105.0 MB 18.2 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 33.5/105.0 MB 17.7 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 34.1/105.0 MB 17.2 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 34.9/105.0 MB 17.2 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 35.5/105.0 MB 16.8 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 36.3/105.0 MB 16.4 MB/s eta 0:00:05\n",
            "   -------------- ------------------------- 37.0/105.0 MB 16.4 MB/s eta 0:00:05\n",
            "   -------------- ------------------------- 37.7/105.0 MB 16.0 MB/s eta 0:00:05\n",
            "   -------------- ------------------------- 38.5/105.0 MB 16.0 MB/s eta 0:00:05\n",
            "   -------------- ------------------------- 39.3/105.0 MB 16.0 MB/s eta 0:00:05\n",
            "   --------------- ------------------------ 40.2/105.0 MB 16.0 MB/s eta 0:00:05\n",
            "   --------------- ------------------------ 40.9/105.0 MB 15.6 MB/s eta 0:00:05\n",
            "   --------------- ------------------------ 41.7/105.0 MB 15.6 MB/s eta 0:00:05\n",
            "   ---------------- ----------------------- 42.6/105.0 MB 16.0 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 43.4/105.0 MB 16.0 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 44.4/105.0 MB 16.8 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 45.3/105.0 MB 17.3 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 46.2/105.0 MB 17.3 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 47.0/105.0 MB 17.7 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 47.9/105.0 MB 18.2 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 48.9/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 49.7/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 50.6/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 51.5/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 52.4/105.0 MB 19.3 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 53.3/105.0 MB 19.3 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 54.1/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 54.9/105.0 MB 19.3 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 55.8/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 56.7/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 57.6/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 58.6/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 59.4/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 60.2/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 61.2/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 62.1/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 62.9/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 63.7/105.0 MB 18.2 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 64.6/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 65.4/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 66.4/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 67.2/105.0 MB 18.7 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 68.1/105.0 MB 18.7 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 69.1/105.0 MB 18.7 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 70.0/105.0 MB 18.7 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 71.0/105.0 MB 18.7 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 71.8/105.0 MB 18.7 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 72.8/105.0 MB 18.7 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 73.6/105.0 MB 19.8 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 74.3/105.0 MB 19.2 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 75.1/105.0 MB 19.2 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 76.0/105.0 MB 19.3 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 76.9/105.0 MB 19.3 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 77.6/105.0 MB 18.7 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 78.6/105.0 MB 18.7 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 79.3/105.0 MB 18.7 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 80.2/105.0 MB 18.7 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 81.0/105.0 MB 18.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 81.9/105.0 MB 18.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 82.8/105.0 MB 18.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 83.4/105.0 MB 17.7 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 84.2/105.0 MB 17.7 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 84.9/105.0 MB 17.7 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 85.5/105.0 MB 17.2 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 86.2/105.0 MB 16.8 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 86.9/105.0 MB 16.8 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 87.6/105.0 MB 16.8 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 88.6/105.0 MB 16.8 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 89.4/105.0 MB 16.8 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 90.4/105.0 MB 16.8 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 91.4/105.0 MB 17.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 92.2/105.0 MB 16.8 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 93.1/105.0 MB 17.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 93.9/105.0 MB 17.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 94.9/105.0 MB 17.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 95.7/105.0 MB 18.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 96.7/105.0 MB 19.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 97.6/105.0 MB 19.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 98.6/105.0 MB 19.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 99.5/105.0 MB 19.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 100.4/105.0 MB 19.3 MB/s eta 0:00:01\n",
            "   ------------------------------------- - 101.4/105.0 MB 19.8 MB/s eta 0:00:01\n",
            "   --------------------------------------  102.3/105.0 MB 19.8 MB/s eta 0:00:01\n",
            "   --------------------------------------  103.4/105.0 MB 19.9 MB/s eta 0:00:01\n",
            "   --------------------------------------  104.2/105.0 MB 19.9 MB/s eta 0:00:01\n",
            "   --------------------------------------  105.0/105.0 MB 20.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  105.0/105.0 MB 20.5 MB/s eta 0:00:01\n",
            "   --------------------------------------- 105.0/105.0 MB 17.2 MB/s eta 0:00:00\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
            "   ---------------------------------------- 116.3/116.3 kB 6.6 MB/s eta 0:00:00\n",
            "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "Downloading aiohttp-3.9.3-cp312-cp312-win_amd64.whl (363 kB)\n",
            "   ---------------------------------------- 0.0/363.4 kB ? eta -:--:--\n",
            "   --------------------------------------- 363.4/363.4 kB 23.5 MB/s eta 0:00:00\n",
            "Downloading huggingface_hub-0.21.3-py3-none-any.whl (346 kB)\n",
            "   ---------------------------------------- 0.0/346.2 kB ? eta -:--:--\n",
            "   --------------------------------------- 346.2/346.2 kB 21.0 MB/s eta 0:00:00\n",
            "Downloading pyarrow-15.0.0-cp312-cp312-win_amd64.whl (25.3 MB)\n",
            "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
            "   - -------------------------------------- 1.0/25.3 MB 20.0 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 1.8/25.3 MB 19.1 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 2.6/25.3 MB 18.3 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 3.5/25.3 MB 18.4 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 4.3/25.3 MB 18.3 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 5.2/25.3 MB 18.3 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 6.1/25.3 MB 18.4 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 6.9/25.3 MB 18.4 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 7.9/25.3 MB 18.6 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 8.8/25.3 MB 18.8 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 9.7/25.3 MB 18.8 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 10.5/25.3 MB 18.7 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 11.4/25.3 MB 18.7 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 12.2/25.3 MB 18.7 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 13.1/25.3 MB 18.7 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 13.9/25.3 MB 18.7 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 14.8/25.3 MB 18.2 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 15.7/25.3 MB 18.7 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 16.6/25.3 MB 18.7 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 17.5/25.3 MB 18.7 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 18.3/25.3 MB 18.2 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 19.1/25.3 MB 18.2 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 20.1/25.3 MB 18.2 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 21.2/25.3 MB 18.7 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 22.1/25.3 MB 19.8 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 23.1/25.3 MB 19.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 24.0/25.3 MB 19.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  25.0/25.3 MB 19.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 25.3/25.3 MB 18.7 MB/s eta 0:00:00\n",
            "Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
            "   ---------------------------------------- 0.0/138.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 138.7/138.7 kB ? eta 0:00:00\n",
            "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "   ---------------------------------------- 0.0/62.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 62.6/62.6 kB 3.5 MB/s eta 0:00:00\n",
            "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading safetensors-0.4.2-cp312-none-win_amd64.whl (270 kB)\n",
            "   ---------------------------------------- 0.0/270.7 kB ? eta -:--:--\n",
            "   --------------------------------------- 270.7/270.7 kB 16.3 MB/s eta 0:00:00\n",
            "Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl (2.2 MB)\n",
            "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "   --------------- ------------------------ 0.8/2.2 MB 17.2 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 1.7/2.2 MB 17.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.2/2.2 MB 17.4 MB/s eta 0:00:00\n",
            "Using cached torch-2.2.1-cp312-cp312-win_amd64.whl (198.5 MB)\n",
            "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
            "   ---------------------------------------- 0.0/146.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 146.7/146.7 kB ? eta 0:00:00\n",
            "Downloading pandas-2.2.1-cp312-cp312-win_amd64.whl (11.5 MB)\n",
            "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.5/11.5 MB 16.8 MB/s eta 0:00:01\n",
            "   ----- ---------------------------------- 1.5/11.5 MB 19.2 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 2.5/11.5 MB 20.0 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 3.5/11.5 MB 20.1 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 4.6/11.5 MB 19.7 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 5.7/11.5 MB 21.6 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 6.7/11.5 MB 21.3 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 7.7/11.5 MB 20.4 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 8.6/11.5 MB 21.2 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 9.8/11.5 MB 21.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 10.9/11.5 MB 22.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.5/11.5 MB 22.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.5/11.5 MB 21.1 MB/s eta 0:00:00\n",
            "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading xxhash-3.4.1-cp312-cp312-win_amd64.whl (29 kB)\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
            "   ---------------------------------------- 60.8/60.8 kB ? eta 0:00:00\n",
            "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "   ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
            "   ---------------------------------------- 163.8/163.8 kB ? eta 0:00:00\n",
            "Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
            "   ---------------------------------------- 0.0/100.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 100.4/100.4 kB ? eta 0:00:00\n",
            "Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
            "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
            "   ---------------------------------------- 50.5/50.5 kB ? eta 0:00:00\n",
            "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
            "   ---------------------------------------- 0.0/61.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 61.6/61.6 kB ? eta 0:00:00\n",
            "Downloading multidict-6.0.5-cp312-cp312-win_amd64.whl (27 kB)\n",
            "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "   ---------------------------------------- 0.0/505.5 kB ? eta -:--:--\n",
            "   --------------------------------------- 505.5/505.5 kB 15.5 MB/s eta 0:00:00\n",
            "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "   ---------------------------------------- 0.0/345.4 kB ? eta -:--:--\n",
            "   --------------------------------------- 345.4/345.4 kB 10.8 MB/s eta 0:00:00\n",
            "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "   ---------------------------------------- 0.0/121.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 121.1/121.1 kB ? eta 0:00:00\n",
            "Downloading yarl-1.9.4-cp312-cp312-win_amd64.whl (76 kB)\n",
            "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 76.4/76.4 kB ? eta 0:00:00\n",
            "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "Using cached MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl (17 kB)\n",
            "Installing collected packages: pytz, xxhash, urllib3, tzdata, safetensors, pyyaml, pyarrow-hotfix, pyarrow, networkx, multidict, MarkupSafe, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, yarl, requests, pandas, multiprocess, jinja2, bitsandbytes, aiosignal, torch, responses, huggingface-hub, aiohttp, tokenizers, accelerate, transformers, datasets, evaluate\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: 'C:\\\\Python312\\\\Scripts\\\\get_gprof'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D1cF8N6xt5J7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Dict, Any\n",
        "import os\n",
        "import json\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9Q6zTg8BuFCX"
      },
      "outputs": [],
      "source": [
        "# You will load this model in a 4-bit inference mode to optimize resource usage. \n",
        "# This setup involves implementing a class named LLM that facilitates loading the model\n",
        "# and generating text completions based on provided prompts\n",
        "class LLM(object):\n",
        "    def __init__(self, model_name=\"codellama/CodeLlama-7b-hf\"):\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
        "        # pass\n",
        "\n",
        "    def generate(self, prompts: List[str], **kwargs) -> List[str]:\n",
        "        self.tokenizer(List[str], return_tensors=\"pt\").to(\"cuda\")\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "59OjAXaewjpt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\grace\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\grace\\.cache\\huggingface\\hub\\models--codellama--CodeLlama-7b-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\grace\\Desktop\\comp3361-asm\\A2\\A2p3.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A2/A2p3.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm \u001b[39m=\u001b[39m LLM()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A2/A2p3.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m llm\u001b[39m.\u001b[39mgenerate([\u001b[39m\"\u001b[39m\u001b[39mA list of colors: red, blue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPortugal is\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "\u001b[1;32mc:\\Users\\grace\\Desktop\\comp3361-asm\\A2\\A2p3.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A2/A2p3.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcodellama/CodeLlama-7b-hf\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A2/A2p3.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name, device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m, load_in_4bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/grace/Desktop/comp3361-asm/A2/A2p3.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name, padding_side\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    560\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 561\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    564\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    565\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:3024\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m     hf_quantizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3023\u001b[0m \u001b[39mif\u001b[39;00m hf_quantizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 3024\u001b[0m     hf_quantizer\u001b[39m.\u001b[39;49mvalidate_environment(\n\u001b[0;32m   3025\u001b[0m         torch_dtype\u001b[39m=\u001b[39;49mtorch_dtype, from_tf\u001b[39m=\u001b[39;49mfrom_tf, from_flax\u001b[39m=\u001b[39;49mfrom_flax, device_map\u001b[39m=\u001b[39;49mdevice_map\n\u001b[0;32m   3026\u001b[0m     )\n\u001b[0;32m   3027\u001b[0m     torch_dtype \u001b[39m=\u001b[39m hf_quantizer\u001b[39m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[0;32m   3028\u001b[0m     device_map \u001b[39m=\u001b[39m hf_quantizer\u001b[39m.\u001b[39mupdate_device_map(device_map)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_environment\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_accelerate_available() \u001b[39mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[1;32m---> 62\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m     63\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m         )\n\u001b[0;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfrom_tf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mor\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfrom_flax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     68\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m sure the weights are in PyTorch format.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m         )\n",
            "\u001b[1;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
          ]
        }
      ],
      "source": [
        "llm = LLM()\n",
        "\n",
        "llm.generate([\"A list of colors: red, blue\", \"Portugal is\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC_-chU1uHqL"
      },
      "outputs": [],
      "source": [
        "class Evaluator(ABC):\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "    @abstractmethod\n",
        "    def load_data(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def build_prompts(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def postprocess_output(self, output: str) -> str:\n",
        "        pass\n",
        "\n",
        "    def generate_completions(self, prompts: List[str], batch_size=4, **kwargs) -> List[str]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def evaluate(self, evalset_file, batch_size=4, save_dir=\"outputs\", max_new_tokens=128, **kwargs):\n",
        "        dataset = self.load_data(evalset_file)\n",
        "        prompts = self.build_prompts(dataset)\n",
        "        outputs = self.generate_completions(prompts, batch_size=batch_size, max_new_tokens=max_new_tokens, **kwargs)\n",
        "\n",
        "        predictions = []\n",
        "        for i, (example, prompt, output) in enumerate(zip(dataset, prompts, outputs)):\n",
        "            prediction = {\n",
        "                \"task_id\": example.get(\"task_id\", f\"task_{i}\"),\n",
        "                \"prompt\": prompt,\n",
        "                \"completion\": self.postprocess_output(output)\n",
        "            }\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        # Save predictions to file\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        prediction_save_path = os.path.join(save_dir, f\"{type(self).__name__}_predictions.jsonl\")\n",
        "        with open(prediction_save_path, \"w\") as fout:\n",
        "            for pred in predictions:\n",
        "                fout.write(json.dumps(pred) + \"\\n\")\n",
        "\n",
        "        # Calculate metrics and print results\n",
        "        results = self.calculate_metrics(predictions, dataset)\n",
        "        print(f\"Results for {type(self).__name__}: {results}\")\n",
        "\n",
        "    @abstractmethod\n",
        "    def calculate_metrics(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8obHGZsMtkoI"
      },
      "source": [
        "## Zero-shot Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEw3OvExutT9"
      },
      "outputs": [],
      "source": [
        "!mkdir -p human_eval\n",
        "!wget -O human_eval/__init__.py https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/human_eval/__init__.py\n",
        "!wget -O human_eval/data.py human_eval https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/human_eval/data.py\n",
        "!wget -O human_eval/evaluation.py https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/human_eval/evaluation.py\n",
        "!wget -O human_eval/execution.py human_eval https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/human_eval/execution.py\n",
        "\n",
        "!mkdir -p data/humaneval\n",
        "!wget -O data/humaneval/HumanEval.jsonl.gz https://github.com/openai/human-eval/raw/master/data/HumanEval.jsonl.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xtsnKpjuKCI"
      },
      "outputs": [],
      "source": [
        "from human_eval.data import read_problems\n",
        "from human_eval.evaluation import evaluate_functional_correctness\n",
        "\n",
        "class HumanEvalEvaluator(Evaluator):\n",
        "    def load_data(self, evalset_file=\"data/humaneval/HumanEval.jsonl.gz\") -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Load the humaneval dataset\n",
        "        :param evalset_file: path to the humaneval dataset file\n",
        "        :return: list of examples\n",
        "        \"\"\"\n",
        "        return list(read_problems(evalset_file).values())\n",
        "\n",
        "    def build_prompts(self, dataset) -> List[str]:\n",
        "        \"\"\"\n",
        "        Build zero-shot prompts from the humaneval dataset.\n",
        "        \"\"\"\n",
        "        prompts = [example[\"prompt\"] for example in dataset]\n",
        "        return prompts\n",
        "\n",
        "    def postprocess_output(self, output: str) -> str:\n",
        "        stop_sequences=[\"\\nclass\", \"\\ndef\", \"\\n#\", \"\\nif\", \"\\nprint\"]\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def calculate_metrics(self, predictions, dataset):\n",
        "        pass_at_k_results = evaluate_functional_correctness(\n",
        "            sample_file=os.path.join(\"outputs\", f\"{type(self).__name__}_predictions.jsonl\"),\n",
        "            k=[1],\n",
        "            problems={example[\"task_id\"]: example for example in dataset},\n",
        "            n_workers=64\n",
        "        )\n",
        "        return pass_at_k_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J91pEUb_uXgB"
      },
      "outputs": [],
      "source": [
        "human_eval_evaluator = HumanEvalEvaluator(llm)\n",
        "human_eval_evaluator.evaluate(batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpDMsRMHtkoJ"
      },
      "source": [
        "## Few-shot Math Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AGOtyjcuPHJ"
      },
      "outputs": [],
      "source": [
        "GSM_EXAMPLARS = [\n",
        "    {\n",
        "        \"question\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n",
        "        \"cot_answer\": \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\\"\\\"\\\"\\n    trees_initial = 15\\n    trees_after = 21\\n    trees_added = trees_after - trees_initial\\n    result = trees_added\\n    return result\",\n",
        "        \"short_answer\": \"6\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n",
        "        \"cot_answer\": \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\\"\\\"\\\"\\n    cars_initial = 3\\n    cars_arrived = 2\\n    total_cars = cars_initial + cars_arrived\\n    result = total_cars\\n    return result\",\n",
        "        \"short_answer\": \"5\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n",
        "        \"cot_answer\": \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\\"\\\"\\\"\\n    leah_chocolates = 32\\n    sister_chocolates = 42\\n    total_chocolates = leah_chocolates + sister_chocolates\\n    chocolates_eaten = 35\\n    chocolates_left = total_chocolates - chocolates_eaten\\n    result = chocolates_left\\n    return result\",\n",
        "        \"short_answer\": \"39\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",\n",
        "        \"cot_answer\": \"Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\\"\\\"\\\"\\n    jason_lollipops_initial = 20\\n    jason_lollipops_after = 12\\n    denny_lollipops = jason_lollipops_initial - jason_lollipops_after\\n    result = denny_lollipops\\n    return result\",\n",
        "        \"short_answer\": \"8\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",\n",
        "        \"cot_answer\": \"Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\\"\\\"\\\"\\n    toys_initial = 5\\n    mom_toys = 2\\n    dad_toys = 2\\n    total_received = mom_toys + dad_toys\\n    total_toys = toys_initial + total_received\\n    result = total_toys\\n    return result\",\n",
        "        \"short_answer\": \"9\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",\n",
        "        \"cot_answer\": \"There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\\"\\\"\\\"\\n    toys_initial = 5\\n    mom_toys = 2\\n    dad_toys = 2\\n    total_received = mom_toys + dad_toys\\n    total_toys = toys_initial + total_received\\n    result = total_toys\\n    return result\",\n",
        "        \"short_answer\": \"29\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",\n",
        "        \"cot_answer\": \"Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\\"\\\"\\\"\\n    golf_balls_initial = 58\\n    golf_balls_lost_tuesday = 23\\n    golf_balls_lost_wednesday = 2\\n    golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday\\n    result = golf_balls_left\\n    return result\",\n",
        "        \"short_answer\": \"33\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\",\n",
        "        \"cot_answer\": \"Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\",\n",
        "        \"pot_answer\": \"def solution():\\n    \\\"\\\"\\\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\\"\\\"\\\"\\n    money_initial = 23\\n    bagels = 5\\n    bagel_cost = 3\\n    money_spent = bagels * bagel_cost\\n    money_left = money_initial - money_spent\\n    result = money_left\\n    return result\",\n",
        "        \"short_answer\": \"8\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbAdW1gMuR0t"
      },
      "outputs": [],
      "source": [
        "class GSM8KEvaluator(Evaluator):\n",
        "    def load_data(self, evalset_file=\"gsm8k\") -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Load the GSM8K dataset https://huggingface.co/datasets/gsm8k with Huggingface datasets library\n",
        "        Load the first 100 examples from the test split in main subset.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build_prompts(self, dataset, n_shot=8, demos=GSM_EXAMPLARS):\n",
        "        \"\"\"\n",
        "        Build few-shot prompts from the GSM8K dataset. Use\n",
        "        :param dataset: list of examples\n",
        "        :param n_shot: number of examples to use for few-shot learning\n",
        "        :param demos: list of demonstrator examples\n",
        "        :return: list of prompts\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def postprocess_output(self, output: str) -> str:\n",
        "        \"\"\"\n",
        "        Postprocess the output from the language model.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def calculate_metrics(self, predictions, dataset):\n",
        "        \"\"\"\n",
        "        Calculate metrics for the GSM8K dataset\n",
        "        \"\"\"\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AngLuranuVVo"
      },
      "outputs": [],
      "source": [
        "gsm8k_evaluator = GSM8KEvaluator(llm)\n",
        "gsm8k_evaluator.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO8otOhhtkoK"
      },
      "source": [
        "## Few-shot Chain-of Thought Math Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkVZ_9oGtkoK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GSM8KCoTEvaluator(GSM8KEvaluator):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYw2dxIJtkoK"
      },
      "outputs": [],
      "source": [
        "gsm8k_cot_evaluator = GSM8KCoTEvaluator(llm)\n",
        "gsm8k_cot_evaluator.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJci4R4mtkoK"
      },
      "source": [
        "## Few-shot Program-of Thought Math Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J94KmfOxtkoK"
      },
      "outputs": [],
      "source": [
        "!pip install timeout-decorator Pebble\n",
        "!wget -O python_executor.py https://raw.githubusercontent.com/ranpox/comp3361-spring2024/main/assignments/A2/python_executor.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7qWrFuqtkoL"
      },
      "outputs": [],
      "source": [
        "from python_executor import PythonExecutor\n",
        "executor = PythonExecutor(get_answer_expr='solution()')\n",
        "\n",
        "codes = [\n",
        "    \"def solution():\\n    return 1 + 1\",\n",
        "    \"def solution():\\n    return 2 * 2\",\n",
        "]\n",
        "\n",
        "predictions = []\n",
        "runtime_errors = []\n",
        "for pred, err in executor.batch_apply(codes):\n",
        "    predictions.append(str(pred))\n",
        "    runtime_errors.append(str(err['exec_info']).strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgRjswkYtkoL"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqZvQnJitkoL"
      },
      "outputs": [],
      "source": [
        "class GSM8KPoTEvaluator(Evaluator):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVw4eXWstkoL"
      },
      "source": [
        "|                    | GSM8K |\n",
        "|--------------------|-------|\n",
        "| Direct Prompting   |       |\n",
        "| Chain-of-Thought   |       |\n",
        "| Program-of-Thought |       |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
